{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJ9oQYZx6rU"
      },
      "source": [
        "---\n",
        "\n",
        "## 🛠️ Use Case: **Customer Support Agent Assistant**\n",
        "\n",
        "### 🎯 Goal:\n",
        "\n",
        "Build an Bigram Language Model that can **assist customer support agents** by:\n",
        "\n",
        "* Understanding customer queries\n",
        "* Suggesting helpful and brand-aligned responses\n",
        "* Learning from past resolved issues\n",
        "\n",
        "This mimics a real-world LLM application in **call centers**, **SaaS platforms**, and **chatbot assistants**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7hmGlh7qo_p"
      },
      "source": [
        "#### **Understanding the Basics**\n",
        "\n",
        "##### 🔎 What is a Language Model?\n",
        "\n",
        "A language model (LM) is a type of neural network that learns to predict the next word or character in a sequence given the previous ones. GPT-style models use a transformer-based architecture for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2a8YNz_t487"
      },
      "source": [
        "####  **Load from HugginFace as Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLoViJSwuCm5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hf://datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0gdUh_Ry32Su",
        "outputId": "8d689961-4b5e-4ac0-a4ad-8de0a2fe1425"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'customer_support_data.txt'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairs = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "  prompt =row['instruction'].strip()\n",
        "  reply = row['response'].strip()\n",
        "\n",
        "  if prompt and reply:\n",
        "    text_pair = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{reply}\\n\\n\"\n",
        "    pairs.append(text_pair)\n",
        "\n",
        "\n",
        "full_text = \"\".join(pairs)\n",
        "\n",
        "\n",
        "output_path = 'customer_support_data.txt'\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(full_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Initialize an empty list**\n",
        "\n",
        "   ```python\n",
        "   pairs = []\n",
        "   ```\n",
        "\n",
        "   *We’ll collect each instruction/response block here.*\n",
        "\n",
        "2. **Loop over every row** in your DataFrame\n",
        "\n",
        "   ```python\n",
        "   for _, row in df.iterrows():\n",
        "   ```\n",
        "\n",
        "   * `_` is the row index (unused).\n",
        "   * `row` holds one conversation example at a time.\n",
        "\n",
        "3. **Extract & clean the text**\n",
        "\n",
        "   ```python\n",
        "   prompt = row['instruction'].strip()\n",
        "   reply  = row['response'].strip()\n",
        "   ```\n",
        "\n",
        "   * `.strip()` removes extra whitespace/newlines.\n",
        "   * Ensures your prompts and replies are tidy.\n",
        "\n",
        "4. **Only keep valid pairs**\n",
        "\n",
        "   ```python\n",
        "   if prompt and reply:\n",
        "   ```\n",
        "\n",
        "   * Skips any examples missing either an instruction or a response.\n",
        "\n",
        "5. **Format as instruction–response blocks**\n",
        "\n",
        "   ```python\n",
        "   text_pair = (\n",
        "       \"### Instruction:\\n\"\n",
        "       f\"{prompt}\\n\\n\"\n",
        "       \"### Response:\\n\"\n",
        "       f\"{reply}\\n\\n\"\n",
        "   )\n",
        "   pairs.append(text_pair)\n",
        "   ```\n",
        "\n",
        "   * Wraps each prompt/reply in clear headers.\n",
        "   * Appends the formatted string to `pairs`.\n",
        "\n",
        "6. **Join all blocks into one string**\n",
        "\n",
        "   ```python\n",
        "   full_text = \"\".join(pairs)\n",
        "   ```\n",
        "\n",
        "   * Creates one continuous text file with all examples back-to-back.\n",
        "\n",
        "7. **Save to disk**\n",
        "\n",
        "   ```python\n",
        "   with open('customer_support_data.txt', \"w\", encoding=\"utf-8\") as f:\n",
        "       f.write(full_text)\n",
        "   ```\n",
        "\n",
        "   * Writes the complete training data to `customer_support_data.txt`.\n",
        "   * Ready for tokenization and model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Read the entire contents of `customer_support_data.txt` into the string variable `text`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VoErdIpL7-XG"
      },
      "outputs": [],
      "source": [
        "text = open('customer_support_data.txt', 'r').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ergmUXcm8ZXt"
      },
      "source": [
        "#### Create Vocabulary (Character-Level)\n",
        "\n",
        "In this step, we build a **character-level vocabulary** — a list of all unique characters that appear in our dataset. This vocabulary will form the basis of our tokenizer, which maps characters to integers and vice versa.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFNktW5l8dDA",
        "outputId": "64b5ba3c-113b-486f-c741-17cb509789cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters:\n",
            " \t\n",
            " !\"#$&'()*+,-./0123456789:;>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz{}¡àé–—’☺✨️🌟👍💡💪🔐🔒🗝😊🙁🙏🛡🤗🤝\n",
            "\n",
            "Vocabulary size: 112\n"
          ]
        }
      ],
      "source": [
        "# Create a sorted list of unique characters (vocabulary)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Output results\n",
        "print(\"Unique characters:\\n\", ''.join(chars))\n",
        "print(\"\\nVocabulary size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbpwcgvjucn5"
      },
      "source": [
        "#### Build Your Tokenizer (Encoder & Decoder)\n",
        "\n",
        "Now that we know *all* the unique characters in our text (our “vocabulary”), we need a way to turn any string of text into numbers—and back again. That’s what a **tokenizer** does.\n",
        "\n",
        "---\n",
        "\n",
        "### 1️⃣ Why Tokenize?\n",
        "\n",
        "* **Neural nets speak numbers**, not letters.\n",
        "* We need a consistent way to map each character to an integer ID.\n",
        "* Later, when we generate text, we’ll convert IDs back into characters.\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ How It Works\n",
        "\n",
        "| Operation  | Input                   | Process                                              | Output                  |\n",
        "| ---------- | ----------------------- | ---------------------------------------------------- | ----------------------- |\n",
        "| **Encode** | `\"hello!\"`              | Look up each character in our `stoi` dictionary      | `[7, 4, 11, 11, 14, 3]` |\n",
        "| **Decode** | `[7, 4, 11, 11, 14, 3]` | Map each ID back to its character in our `itos` dict | `\"hello!\"`              |\n",
        "\n",
        "* **`stoi`** = **S**tring → **I**nteger map\n",
        "* **`itos`** = **I**nteger → **S**tring map\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ Why Character-Level?\n",
        "\n",
        "* **Simplicity:** Easy to inspect and debug—every token is one character.\n",
        "* **Transparency:** You see exactly how “a”, “b”, “!” and “4” get their own IDs.\n",
        "* **Good for small models:** No giant vocabularies, no complex subword merges.\n",
        "\n",
        "---\n",
        "\n",
        "### 4️⃣ (Bonus) Word-Piece / BPE Alternative\n",
        "\n",
        "If you ever want to scale up to word-pieces like GPT-2 uses:\n",
        "\n",
        "```python\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "print(\"GPT-2 uses\", enc.n_vocab, \"subword tokens\")\n",
        "print(\"Example:\", enc.encode(\"hello world\"))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aILteFz92g6",
        "outputId": "508ccb70-bdd5-46ff-c7fe-c69a660b3555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: LLM\n",
            "Encoded: [43, 43, 44]\n",
            "Decoded: LLM\n"
          ]
        }
      ],
      "source": [
        "# Build character-to-index and index-to-character mappings\n",
        "stoi = { ch:i for i, ch in enumerate(chars) }  # String to Integer\n",
        "itos = { i:ch for i, ch in enumerate(chars) }  # Integer to String\n",
        "\n",
        "# Define encoder and decoder functions\n",
        "encode = lambda s: [stoi[c] for c in s]        # Converts string to list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Converts list of integers back to string\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"LLM\"\n",
        "encoded = encode(sample_text)\n",
        "decoded = decode(encoded)\n",
        "\n",
        "print(\"Original text:\", sample_text)\n",
        "print(\"Encoded:\", encoded)\n",
        "print(\"Decoded:\", decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7Ej_GHyu73a"
      },
      "source": [
        "#### Encode the Entire Dataset\n",
        "\n",
        "Now that we’ve built our **encode** and **decode** functions, it’s time to turn **all** of our raw text into numbers—one big sequence of token IDs that our model can train on.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Why Encode the Full Text?\n",
        "\n",
        "* **Batching & slicing** require numeric tensors, not strings.\n",
        "* Converting once up front is more efficient than encoding on the fly.\n",
        "* The resulting tensor is the “source of truth” for all downstream data splits and training loops.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtBu2PsA_fyZ",
        "outputId": "69c28591-8f89-4760-dad0-55838f0ce7f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: torch.Size([19240191])\n",
            "Data type: torch.int64\n",
            "First 1000 tokens:\n",
            " tensor([ 5,  5,  5,  2, 40, 75, 80, 81, 79, 82, 64, 81, 70, 76, 75, 27,  1, 78,\n",
            "        82, 66, 80, 81, 70, 76, 75,  2, 62, 63, 76, 82, 81,  2, 64, 62, 75, 64,\n",
            "        66, 73, 73, 70, 75, 68,  2, 76, 79, 65, 66, 79,  2, 88, 88, 46, 79, 65,\n",
            "        66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89,  1,  1,  5,  5,  5,  2, 49,\n",
            "        66, 80, 77, 76, 75, 80, 66, 27,  1, 40,  8, 83, 66,  2, 82, 75, 65, 66,\n",
            "        79, 80, 81, 76, 76, 65,  2, 86, 76, 82,  2, 69, 62, 83, 66,  2, 62,  2,\n",
            "        78, 82, 66, 80, 81, 70, 76, 75,  2, 79, 66, 68, 62, 79, 65, 70, 75, 68,\n",
            "         2, 64, 62, 75, 64, 66, 73, 70, 75, 68,  2, 76, 79, 65, 66, 79,  2, 88,\n",
            "        88, 46, 79, 65, 66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89, 13,  2, 62,\n",
            "        75, 65,  2, 40,  8, 74,  2, 69, 66, 79, 66,  2, 81, 76,  2, 77, 79, 76,\n",
            "        83, 70, 65, 66,  2, 86, 76, 82,  2, 84, 70, 81, 69,  2, 81, 69, 66,  2,\n",
            "        70, 75, 67, 76, 79, 74, 62, 81, 70, 76, 75,  2, 86, 76, 82,  2, 75, 66,\n",
            "        66, 65, 15,  2, 47, 73, 66, 62, 80, 66,  2, 68, 76,  2, 62, 69, 66, 62,\n",
            "        65,  2, 62, 75, 65,  2, 62, 80, 72,  2, 86, 76, 82, 79,  2, 78, 82, 66,\n",
            "        80, 81, 70, 76, 75, 13,  2, 62, 75, 65,  2, 40,  8, 73, 73,  2, 65, 76,\n",
            "         2, 74, 86,  2, 63, 66, 80, 81,  2, 81, 76,  2, 62, 80, 80, 70, 80, 81,\n",
            "         2, 86, 76, 82, 15,  1,  1,  5,  5,  5,  2, 40, 75, 80, 81, 79, 82, 64,\n",
            "        81, 70, 76, 75, 27,  1, 70,  2, 69, 62, 83, 66,  2, 62,  2, 78, 82, 66,\n",
            "        80, 81, 70, 76, 75,  2, 62, 63, 76, 82, 81,  2, 64, 62, 75, 64, 66, 73,\n",
            "        73, 70, 75, 68,  2, 76, 76, 79, 65, 66, 79,  2, 88, 88, 46, 79, 65, 66,\n",
            "        79,  2, 45, 82, 74, 63, 66, 79, 89, 89,  1,  1,  5,  5,  5,  2, 49, 66,\n",
            "        80, 77, 76, 75, 80, 66, 27,  1, 40,  8, 83, 66,  2, 63, 66, 66, 75,  2,\n",
            "        70, 75, 67, 76, 79, 74, 66, 65,  2, 81, 69, 62, 81,  2, 86, 76, 82,  2,\n",
            "        69, 62, 83, 66,  2, 62,  2, 78, 82, 66, 80, 81, 70, 76, 75,  2, 62, 63,\n",
            "        76, 82, 81,  2, 64, 62, 75, 64, 66, 73, 70, 75, 68,  2, 76, 79, 65, 66,\n",
            "        79,  2, 88, 88, 46, 79, 65, 66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89,\n",
            "        15,  2, 40,  8, 74,  2, 69, 66, 79, 66,  2, 81, 76,  2, 62, 80, 80, 70,\n",
            "        80, 81,  2, 86, 76, 82,  3,  2, 47, 73, 66, 62, 80, 66,  2, 68, 76,  2,\n",
            "        62, 69, 66, 62, 65,  2, 62, 75, 65,  2, 73, 66, 81,  2, 74, 66,  2, 72,\n",
            "        75, 76, 84,  2, 84, 69, 62, 81,  2, 80, 77, 66, 64, 70, 67, 70, 64,  2,\n",
            "        78, 82, 66, 80, 81, 70, 76, 75,  2, 86, 76, 82,  2, 69, 62, 83, 66, 13,\n",
            "         2, 62, 75, 65,  2, 40,  8, 73, 73,  2, 77, 79, 76, 83, 70, 65, 66,  2,\n",
            "        86, 76, 82,  2, 84, 70, 81, 69,  2, 62, 73, 73,  2, 81, 69, 66,  2, 70,\n",
            "        75, 67, 76, 79, 74, 62, 81, 70, 76, 75,  2, 62, 75, 65,  2, 68, 82, 70,\n",
            "        65, 62, 75, 64, 66,  2, 86, 76, 82,  2, 75, 66, 66, 65, 15,  2, 56, 76,\n",
            "        82, 79,  2, 80, 62, 81, 70, 80, 67, 62, 64, 81, 70, 76, 75,  2, 70, 80,\n",
            "         2, 74, 86,  2, 81, 76, 77,  2, 77, 79, 70, 76, 79, 70, 81, 86, 15,  1,\n",
            "         1,  5,  5,  5,  2, 40, 75, 80, 81, 79, 82, 64, 81, 70, 76, 75, 27,  1,\n",
            "        70,  2, 75, 66, 66, 65,  2, 69, 66, 73, 77,  2, 64, 62, 75, 64, 66, 73,\n",
            "        73, 70, 75, 68,  2, 77, 82, 64, 69, 62, 80, 66,  2, 88, 88, 46, 79, 65,\n",
            "        66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89,  1,  1,  5,  5,  5,  2, 49,\n",
            "        66, 80, 77, 76, 75, 80, 66, 27,  1, 40,  2, 64, 62, 75,  2, 80, 66, 75,\n",
            "        80, 66,  2, 81, 69, 62, 81,  2, 86, 76, 82,  8, 79, 66,  2, 80, 66, 66,\n",
            "        72, 70, 75, 68,  2, 62, 80, 80, 70, 80, 81, 62, 75, 64, 66,  2, 84, 70,\n",
            "        81, 69,  2, 64, 62, 75, 64, 66, 73, 70, 75, 68,  2, 86, 76, 82, 79,  2,\n",
            "        77, 82, 79, 64, 69, 62, 80, 66,  2, 84, 70, 81, 69,  2, 81, 69, 66,  2,\n",
            "        77, 82, 79, 64, 69, 62, 80, 66,  2, 75, 82, 74, 63, 66, 79,  2, 88, 88,\n",
            "        46, 79, 65, 66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89, 15,  2, 40,  2,\n",
            "        62, 77, 76, 73, 76, 68, 70, 87, 66,  2, 67, 76, 79,  2, 62, 75, 86,  2,\n",
            "        70, 75, 64, 76, 75, 83, 66, 75, 70, 66, 75, 64, 66,  2, 64, 62, 82, 80,\n",
            "        66, 65, 13,  2, 62, 75, 65,  2, 40,  8, 74,  2, 69, 66, 79, 66,  2, 81,\n",
            "        76,  2, 68, 82, 70, 65, 66,  2, 86, 76, 82,  2, 81, 69, 79, 76, 82, 68,\n",
            "        69,  2, 81, 69, 66,  2, 77, 79, 76, 64, 66, 80, 80, 15,  1,  1, 51, 76,\n",
            "         2, 64, 62, 75, 64, 66, 73,  2, 86, 76, 82, 79,  2, 77, 82, 79, 64, 69,\n",
            "        62, 80, 66, 13,  2, 77, 73, 66, 62, 80, 66,  2, 67, 76, 73, 73, 76, 84,\n",
            "         2, 81, 69, 66, 80, 66,  2, 80, 81, 66])\n"
          ]
        }
      ],
      "source": [
        "import torch  # We use PyTorch: https://pytorch.org\n",
        "\n",
        "# Encode the entire text using our character-level encoder\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Print tensor information\n",
        "print(\"Data shape:\", data.shape)\n",
        "print(\"Data type:\", data.dtype)\n",
        "\n",
        "# Preview first 1000 encoded tokens\n",
        "print(\"First 1000 tokens:\\n\", data[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiB79ZL1vbMn"
      },
      "source": [
        "#### Train/Test Split\n",
        "\n",
        "Split data into training (90%) and validation (10%) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mk8nbWH0vdpx"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwDhFT46vqVj"
      },
      "source": [
        "#### **Creating Mini‐Batches from chunks for Training**\n",
        "Instead of training on one long sequence at a time, we cut our data into many small windows—then group those windows into mini‐batches so the model learns faster and more stably.\n",
        "\n",
        "#### 1️⃣ Why Mini‐Batches?\n",
        "\n",
        "* **Efficiency:** GPUs work best on parallel data.\n",
        "* **Stability:** Averaging the loss over multiple windows smooths out noisy gradients.\n",
        "* **Coverage:** Random windows from across the text expose the model to more diverse patterns each step.\n",
        "\n",
        "\n",
        "#### Full data (token IDs):\n",
        "\n",
        "  ```\n",
        "  data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, …]\n",
        "  ```\n",
        "* **block\\_size** = 4\n",
        "* **Batch size** (B) = 3\n",
        "\n",
        "##### Picking a Chunk\n",
        "\n",
        "Each chunk will be length `block_size + 1 = 5`.\n",
        "\n",
        "chunk = [0, 1, 2, 3, 4]\n",
        "\n",
        "\n",
        "#### Extracting `x`, and `y` from chunk\n",
        "\n",
        "* **`chunk`**: 5 tokens including one extra for alignment.\n",
        "* **`x[b]`**: the **context window** of length 4 - [:block_size]\n",
        "* **`y[b]`**: the **next-token labels**, shifted by one - [1:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-LMsqFwCvUx",
        "outputId": "c88f96a2-e758-46b0-c99c-639882fa9385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Sample chunk of tokens: [5, 5, 5, 2, 40, 75, 80, 81, 79]\n",
            "\n",
            "🔁 Next-token prediction learning:\n",
            "📚 When input is [5] → target is: 5\n",
            "📚 When input is [5, 5] → target is: 5\n",
            "📚 When input is [5, 5, 5] → target is: 2\n",
            "📚 When input is [5, 5, 5, 2] → target is: 40\n",
            "📚 When input is [5, 5, 5, 2, 40] → target is: 75\n",
            "📚 When input is [5, 5, 5, 2, 40, 75] → target is: 80\n",
            "📚 When input is [5, 5, 5, 2, 40, 75, 80] → target is: 81\n",
            "📚 When input is [5, 5, 5, 2, 40, 75, 80, 81] → target is: 79\n"
          ]
        }
      ],
      "source": [
        "block_size = 8  # This is the context window length\n",
        "chunk = train_data[:block_size + 1]  # One extra token to create input-output alignment\n",
        "print(\"📦 Sample chunk of tokens:\", chunk.tolist())\n",
        "\n",
        "# Create input (x) and target (y) sequences\n",
        "x = chunk[:block_size]            # inputs\n",
        "y = chunk[1:block_size + 1]       # targets (shifted by one)\n",
        "\n",
        "\n",
        "print(\"\\n🔁 Next-token prediction learning:\")\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"📚 When input is {context.tolist()} → target is: {target.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create Data Batches\n",
        "\n",
        "* **Full data** (token IDs):\n",
        "\n",
        "  ```\n",
        "  data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, …]\n",
        "  ```\n",
        "* **block\\_size** = 4\n",
        "* **Batch size** (B) = 3\n",
        "\n",
        "---\n",
        "\n",
        "### 1️⃣ Picking Random Chunks\n",
        "\n",
        "Suppose we randomly choose start indices:\n",
        "\n",
        "```\n",
        "start_idxs = [2, 5, 8]\n",
        "```\n",
        "\n",
        "Each chunk will be length `block_size + 1 = 5`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ Extracting `x`, and `y` from `chunk`\n",
        "\n",
        "| Batch b | start\\_idx | chunk = data\\[i\\:i+5] | x\\[b] = first 4 tokens | y\\[b] = last 4 tokens |\n",
        "| :-----: | :--------: | :-------------------: | :--------------------: | :-------------------: |\n",
        "|  **0**  |      2     |   `[2, 3, 4, 5, 6]`   |     `[2, 3, 4, 5]`     |     `[3, 4, 5, 6]`    |\n",
        "|  **1**  |      5     |   `[5, 6, 7, 8, 9]`   |     `[5, 6, 7, 8]`     |     `[6, 7, 8, 9]`    |\n",
        "|  **2**  |      8     |  `[8, 9, 10, 11, 12]` |    `[8, 9, 10, 11]`    |   `[9, 10, 11, 12]`   |\n",
        "\n",
        "* **`chunk`**: 5 tokens including one extra for alignment.\n",
        "* **`x[b]`**: the **context window** of length 4.\n",
        "* **`y[b]`**: the **next-token labels**, shifted by one.\n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ Resulting Mini‐Batch\n",
        "\n",
        "* `x` has shape `(3, 4)` and looks like:\n",
        "\n",
        "  ```\n",
        "  [\n",
        "    [2,  3,  4,  5],\n",
        "    [5,  6,  7,  8],\n",
        "    [8,  9, 10, 11],\n",
        "  ]\n",
        "  ```\n",
        "* `y` has shape `(3, 4)` and looks like:\n",
        "\n",
        "  ```\n",
        "  [\n",
        "    [3,  4,  5,  6],\n",
        "    [6,  7,  8,  9],\n",
        "    [9, 10, 11, 12],\n",
        "  ]\n",
        "  ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EImVNIpOFKU9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4   # How many sequences to process in parallel\n",
        "block_size = 8   # Length of each sequence (context window)\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    Samples a mini-batch of input (x) and target (y) sequences from the dataset.\n",
        "\n",
        "    Args:\n",
        "        split (str): One of 'train' or 'val' to choose the dataset split.\n",
        "\n",
        "    Returns:\n",
        "        x (torch.Tensor): Input sequences of shape (batch_size, block_size)\n",
        "        y (torch.Tensor): Target sequences of shape (batch_size, block_size)\n",
        "                          Each y[i, t] is the next character after x[i, t]\n",
        "    \"\"\"\n",
        "    assert split in ['train', 'val'], \"split must be 'train' or 'val'\"\n",
        "\n",
        "    data_source = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Randomly sample starting indices for each sequence\n",
        "    start_indices = torch.randint(0, len(data_source) - block_size, (batch_size,))\n",
        "\n",
        "    # Build input and target tensors using slicing\n",
        "    x = torch.stack([data_source[i:i + block_size] for i in start_indices])\n",
        "    y = torch.stack([data_source[i + 1:i + block_size + 1] for i in start_indices])\n",
        "\n",
        "    return x, y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8SxIkVqFOeB",
        "outputId": "a471913c-8c20-43e7-bc00-7d60023e79a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧮 Input batch shape: torch.Size([4, 8])\n",
            "🧮 Target batch shape: torch.Size([4, 8])\n",
            "\n",
            "🧾 Inputs (xb):\n",
            "tensor([[86, 76, 82,  2, 64, 62, 75,  2],\n",
            "        [ 2, 70, 75, 67, 76, 79, 74, 62],\n",
            "        [84, 76, 79, 65,  2, 67, 76, 79],\n",
            "        [ 2, 80, 77, 66, 64, 70, 67, 70]])\n",
            "\n",
            "🎯 Targets (yb):\n",
            "tensor([[76, 82,  2, 64, 62, 75,  2, 80],\n",
            "        [70, 75, 67, 76, 79, 74, 62, 81],\n",
            "        [76, 79, 65,  2, 67, 76, 79,  2],\n",
            "        [80, 77, 66, 64, 70, 67, 70, 64]])\n"
          ]
        }
      ],
      "source": [
        "# Generate a training batch\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "# Inspect the shape of the input and target tensors\n",
        "print(\"🧮 Input batch shape:\", xb.shape)   # Expected: (4, 8)\n",
        "print(\"🧮 Target batch shape:\", yb.shape) # Expected: (4, 8)\n",
        "\n",
        "# View actual data\n",
        "print(\"\\n🧾 Inputs (xb):\")\n",
        "print(xb)\n",
        "\n",
        "print(\"\\n🎯 Targets (yb):\")\n",
        "print(yb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW8N9v_3T6ep"
      },
      "source": [
        "#### 🧠 Why Do We Train This Way?\n",
        "\n",
        "This is called **next-token prediction** — the foundational idea behind models like GPT.\n",
        "\n",
        "At each position in a sequence, the model learns to predict **what character comes next** based on the context it has seen so far.\n",
        "\n",
        "* In this example:\n",
        "\n",
        "  * Feed the model `[86]` → expect it to predict `76`\n",
        "  * Feed `[86, 76]` → expect `82`\n",
        "  * Feed `[86, 76, 82]` → expect `2`\n",
        "  * ... and so on\n",
        "\n",
        "\n",
        "### 📚 What the Model Learns Over Time\n",
        "\n",
        "By seeing **many random sequences from across the dataset**, the model learns:\n",
        "\n",
        "* Patterns in character sequences\n",
        "* What tokens frequently follow others\n",
        "* How to continue a sentence or phrase\n",
        "* Eventually — how to generate coherent text from scratch\n",
        "\n",
        "This chunking and batching strategy is the **core training loop** of autoregressive language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp-e_ebwvxRn"
      },
      "source": [
        "#### Build the Model - Bigram Language Model**\n",
        "\n",
        "### ⚙️ Step 9: Define a Simple Model\n",
        "\n",
        "In this step, we define and test a **Bigram Language Model** — the simplest type of autoregressive model. It predicts the next token **only** based on the current token, without considering any context before it.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 What Is a Bigram Model?\n",
        "\n",
        "A **bigram model** looks at just **one character (token)** and tries to predict **what comes next**. For example:\n",
        "- Given `'H'`, predict `'e'`\n",
        "- Given `'e'`, predict `'l'`\n",
        "- Given `'l'`, predict `'l'`\n",
        "- Given `'l'`, predict `'o'`\n",
        "\n",
        "This is the minimal form of a language model — it **ignores all previous context** except the current token.\n",
        "\n",
        "---\n",
        "\n",
        "### `token_embedding_table`\n",
        "\n",
        "\n",
        "* We define a PyTorch `nn.Module`.\n",
        "* Inside, we create a simple `nn.Embedding` layer.\n",
        "\n",
        "  * This is a **lookup table** with shape `(vocab_size, vocab_size)`\n",
        "  * Each token maps directly to a row — a vector of logits predicting the next token.\n",
        "\n",
        "\n",
        "**Defining the Embedding Layer**\n",
        "\n",
        "Assuming we have a vocab_size of 5\n",
        "\n",
        "```python\n",
        "self.token_embedding_table = nn.Embedding(5, 5)\n",
        "```\n",
        "\n",
        "This creates a **learnable** weight matrix of shape `(5 × 5)`. Internally, PyTorch initializes it (e.g. randomly) as follows:\n",
        "\n",
        "| **Token ID** | **Dim 0** | **Dim 1** | **Dim 2** | **Dim 3** | **Dim 4** |\n",
        "| :----------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n",
        "|     **0**    |    0.12   |   –0.34   |    0.45   |   –0.01   |    0.33   |\n",
        "|     **1**    |   –0.05   |    0.78   |   –1.10   |    0.22   |    0.09   |\n",
        "|     **2**    |    1.30   |   –0.44   |    0.00   |    0.55   |   –0.88   |\n",
        "|     **3**    |   –0.77   |    0.21   |    0.64   |   –0.19   |    1.15   |\n",
        "|     **4**    |    0.03   |   –0.67   |   –0.40   |    0.90   |   –0.12   |\n",
        "\n",
        "* **Rows** correspond to **token IDs** (0–4).\n",
        "* **Columns** are the **embedding dimensions**, each of which will serve as the raw “logit score” for predicting the next token when you use this embedding in a Bigram model.\n",
        "\n",
        "---\n",
        "### `The Input Indices (`idx`)`\n",
        "\n",
        "Suppose during training you form a mini-batch of **B = 2** sequences, each of length **T = 3**. You might have:\n",
        "\n",
        "```\n",
        "idx = [\n",
        "  [2, 0, 4],   # ← Batch 0’s token IDs at time-steps 0,1,2\n",
        "  [1, 3, 2],   # ← Batch 1’s token IDs\n",
        "]\n",
        "```\n",
        "\n",
        "Shape: `(2, 3)`\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Looking Up Embeddings\n",
        "\n",
        "When you call:\n",
        "\n",
        "```python\n",
        "logits = self.token_embedding_table(idx)\n",
        "```\n",
        "\n",
        "PyTorch “gathers” the corresponding rows from the 5×5 table for each position in `idx`, producing a tensor of shape **(B, T, 5)**. Concretely:\n",
        "\n",
        "### Batch 0 (`idx[0] = [2,0,4]`)\n",
        "\n",
        "| Time-step (t) | token ID | Embedding Vector (Dim 0–4)           |\n",
        "| ------------: | :------: | :----------------------------------- |\n",
        "|         **0** |     2    | \\[ 1.30, –0.44,  0.00,  0.55, –0.88] |\n",
        "|         **1** |     0    | \\[ 0.12, –0.34,  0.45, –0.01,  0.33] |\n",
        "|         **2** |     4    | \\[ 0.03, –0.67, –0.40,  0.90, –0.12] |\n",
        "\n",
        "### Batch 1 (`idx[1] = [1,3,2]`)\n",
        "\n",
        "| Time-step (t) | token ID | Embedding Vector (Dim 0–4)           |\n",
        "| ------------: | :------: | :----------------------------------- |\n",
        "|         **0** |     1    | \\[–0.05,  0.78, –1.10,  0.22,  0.09] |\n",
        "|         **1** |     3    | \\[–0.77,  0.21,  0.64, –0.19,  1.15] |\n",
        "|         **2** |     2    | \\[ 1.30, –0.44,  0.00,  0.55, –0.88] |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Resulting `logits` Tensor\n",
        "\n",
        "Putting those together, `logits` is:\n",
        "\n",
        "```\n",
        "logits = [\n",
        "  [  # Batch 0 (shape: 3×5)\n",
        "    [1.30, –0.44,  0.00,  0.55, –0.88],  # t=0\n",
        "    [0.12, –0.34,  0.45, –0.01,  0.33],  # t=1\n",
        "    [0.03, –0.67, –0.40,  0.90, –0.12],  # t=2\n",
        "  ],\n",
        "  [  # Batch 1 (shape: 3×5)\n",
        "    [–0.05,  0.78, –1.10,  0.22,  0.09], # t=0\n",
        "    [–0.77,  0.21,  0.64, –0.19,  1.15], # t=1\n",
        "    [ 1.30, –0.44,  0.00,  0.55, –0.88], # t=2\n",
        "  ]\n",
        "] #(shape: 2x3×5 = BxTxC)\n",
        "```\n",
        "\n",
        "Shape: **(2, 3, 5)** in our toy; in your real model it’s **(B, T, vocab\\_size)**.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🎯 What Are Logits?\n",
        "\n",
        "* They are **raw scores** for all possible next tokens.\n",
        "* Later used in `cross_entropy()` to calculate loss.\n",
        "* Think of them as the model saying:\n",
        "\n",
        "  > “Here are my 112 guesses for what comes after this token.”\n",
        "\n",
        "---\n",
        "\n",
        "### `Loss Calculation`\n",
        "\n",
        "* PyTorch's `cross_entropy` expects:\n",
        "\n",
        "  * `logits` to be `(N, C)` → N is the number of predictions, C is the number of classes\n",
        "  * `targets` to be a 1D tensor of shape `(N,)`\n",
        "* So we reshape the tensors:\n",
        "\n",
        "  * `B*T` is how many total predictions we made\n",
        "* The result is a **single scalar loss** that tells how well the model predicts the next tokens across the batch.\n",
        "\n",
        "### Recall Our Toy Example\n",
        "\n",
        "* **Batch size (B)** = 2\n",
        "* **Time-steps (T)** = 3\n",
        "* **Classes (C)** = 5\n",
        "\n",
        "```python\n",
        "# (B, T) input indices\n",
        "idx = [\n",
        "  [2, 0, 4],   # Batch 0\n",
        "  [1, 3, 2],   # Batch 1\n",
        "]\n",
        "\n",
        "# After embedding lookup → (B, T, C) logits:\n",
        "logits = [\n",
        "  [  # Batch 0\n",
        "    [1.30, –0.44,  0.00,  0.55, –0.88],  # t=0\n",
        "    [0.12, –0.34,  0.45, –0.01,  0.33],  # t=1\n",
        "    [0.03, –0.67, –0.40,  0.90, –0.12],  # t=2\n",
        "  ],\n",
        "  [  # Batch 1\n",
        "    [–0.05,  0.78, –1.10,  0.22,  0.09], # t=0\n",
        "    [–0.77,  0.21,  0.64, –0.19,  1.15], # t=1\n",
        "    [ 1.30, –0.44,  0.00,  0.55, –0.88], # t=2\n",
        "  ]\n",
        "]\n",
        "```\n",
        "\n",
        "Suppose our **targets** tensor is the “true” next-token IDs for each position:\n",
        "\n",
        "```python\n",
        "targets = [\n",
        "  [0, 2, 1],   # Batch 0’s true next-token labels\n",
        "  [3, 4, 0],   # Batch 1’s labels\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Shapes Before Flattening\n",
        "\n",
        "| Tensor    | Shape       | Description                                   |\n",
        "| --------- | ----------- | --------------------------------------------- |\n",
        "| `logits`  | `(2, 3, 5)` | 2 batches × 3 time-steps × 5 class-scores     |\n",
        "| `targets` | `(2, 3)`    | 2 batches × 3 true labels (one per time-step) |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Flattening to Fit `F.cross_entropy`\n",
        "\n",
        "PyTorch’s **`cross_entropy`** expects:\n",
        "\n",
        "* **`logits`** shape `(N, C)`\n",
        "* **`targets`** shape `(N,)`\n",
        "\n",
        "where **N** is the total number of predictions (`B × T`). We flatten accordingly:\n",
        "\n",
        "```python\n",
        "B, T, C = logits.shape              # B=2, T=3, C=5\n",
        "\n",
        "logits = logits.view(B * T, C)      # → shape (6, 5)\n",
        "targets = targets.view(B * T)       # → shape (6,)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. How the Flattening Works\n",
        "\n",
        "| (batch, time) | Flat index n | Logits row (length 5)               | Target label |\n",
        "| ------------: | -----------: | ----------------------------------- | -----------: |\n",
        "|        (0, 0) |            0 | \\[1.30, –0.44,  0.00,  0.55, –0.88] |            0 |\n",
        "|        (0, 1) |            1 | \\[0.12, –0.34,  0.45, –0.01,  0.33] |            2 |\n",
        "|        (0, 2) |            2 | \\[0.03, –0.67, –0.40,  0.90, –0.12] |            1 |\n",
        "|        (1, 0) |            3 | \\[–0.05, 0.78, –1.10, 0.22,  0.09]  |            3 |\n",
        "|        (1, 1) |            4 | \\[–0.77, 0.21,  0.64, –0.19, 1.15]  |            4 |\n",
        "|        (1, 2) |            5 | \\[ 1.30, –0.44, 0.00,  0.55, –0.88] |            0 |\n",
        "\n",
        "* **Rows 0–5** of the flattened `logits` correspond to each `(batch, time)` pair in row-major order.\n",
        "* The flattened `targets` vector is `[0, 2, 1, 3, 4, 0]`.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Computing the Loss\n",
        "\n",
        "With shapes `(6, 5)` and `(6,)`, we can now call:\n",
        "\n",
        "```python\n",
        "loss = F.cross_entropy(logits, targets)\n",
        "```\n",
        "\n",
        "* For each of the 6 rows, `cross_entropy`\n",
        "\n",
        "  1. applies `softmax` over the 5 class-scores\n",
        "  2. picks out the probability of the true class (from `targets`)\n",
        "  3. computes `–log(p_true)`\n",
        "* Finally, it **averages** these 6 values into a single scalar loss.\n",
        "\n",
        "\n",
        "### 📉 What `loss` Gives You:\n",
        "\n",
        "* A single **scalar value** (e.g., `4.87`)\n",
        "* Measures **how wrong** the model’s predictions are\n",
        "* Lower is better! (ideal random loss ≈ `-ln(1/vocab_size)`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### `Generate Method (Sampling Text)`\n",
        "\n",
        "\n",
        "#### Setup Recap\n",
        "\n",
        "**Embedding table** (5×5) from before:\n",
        "\n",
        "| Token ID | Dim 0 | Dim 1 | Dim 2 | Dim 3 | Dim 4 |\n",
        "| :------: | :---: | :---: | :---: | :---: | :---: |\n",
        "|   **0**  |  0.12 | –0.34 |  0.45 | –0.01 |  0.33 |\n",
        "|   **1**  | –0.05 |  0.78 | –1.10 |  0.22 |  0.09 |\n",
        "|   **2**  |  1.30 | –0.44 |  0.00 |  0.55 | –0.88 |\n",
        "|   **3**  | –0.77 |  0.21 |  0.64 | –0.19 |  1.15 |\n",
        "|   **4**  |  0.03 | –0.67 | –0.40 |  0.90 | –0.12 |\n",
        "\n",
        "**Initial `idx`** (shape (2, 3)):\n",
        "\n",
        "| Batch | t=0 | t=1 | t=2 |\n",
        "| :---: | :-: | :-: | :-: |\n",
        "| **0** |  2  |  0  |  4  |\n",
        "| **1** |  1  |  3  |  2  |\n",
        "\n",
        "---\n",
        "\n",
        "## Steps for Generation\n",
        "\n",
        "1. **Lookup logits** for the **last** token in each batch (t=2):\n",
        "\n",
        "   * Batch 0 last ID = 4 → logits = embedding row 4 = `[0.03, –0.67, –0.40, 0.90, –0.12]`\n",
        "   * Batch 1 last ID = 2 → logits = embedding row 2 = `[1.30, –0.44, 0.00, 0.55, –0.88]`\n",
        "\n",
        "2. **Argmax** (greedy) picks the highest logit:\n",
        "\n",
        "   * Batch 0 → max at **Dim 3** (0.90) → new token = 3\n",
        "   * Batch 1 → max at **Dim 0** (1.30) → new token = 0\n",
        "\n",
        "3. **Append** to each sequence:\n",
        "\n",
        "| Batch | Before     | After append (t=3) |\n",
        "| :---: | :--------- | :----------------- |\n",
        "| **0** | \\[2, 0, 4] | \\[2, 0, 4, **3**]  |\n",
        "| **1** | \\[1, 3, 2] | \\[1, 3, 2, **0**]  |\n",
        "\n",
        "Now `idx.shape == (2, 4)`.\n",
        "\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "Starting from\n",
        "\n",
        "```\n",
        "idx = [\n",
        "  [2, 0, 4],\n",
        "  [1, 3, 2],\n",
        "]\n",
        "```\n",
        "\n",
        "and running `generate(idx, max_new_tokens=2)` (greedy):\n",
        "\n",
        "1. **Step 1** → append **3** to batch 0, **0** to batch 1\n",
        "\n",
        "Yields:\n",
        "\n",
        "```\n",
        "idx_out = [\n",
        "  [2, 0, 4, 3],\n",
        "  [1, 3, 2, 0],\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rilUd-q4v0mh",
        "outputId": "b5ab85e7-03fa-44f8-f031-137c92795213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape: torch.Size([32, 112])\n",
            "Loss: tensor(5.2504, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Generated text (raw tokens): [0, 6, 39, 103, 40, 60, 108, 103, 103, 37, 31, 46, 35, 12, 104, 62, 29, 43, 64, 3, 14, 36, 84, 35, 27, 13, 60, 15, 99, 96, 93, 45, 32, 74, 44, 14, 25, 99, 18, 53, 7, 34, 77, 1, 65, 91, 62, 63, 46, 26, 81, 28, 91, 65, 90, 84, 103, 59, 16, 38, 104, 33, 65, 59, 82, 9, 29, 85, 33, 77, 19, 36, 9, 36, 68, 25, 0, 62, 45, 98, 80, 41, 38, 107, 24, 14, 111, 13, 44, 62, 88, 34, 36, 68, 107, 34, 77, 61, 83, 80, 76]\n",
            "\n",
            "Generated text (decoded):\n",
            "\t$H🔐I_🙏🔐🔐F@OD+🔒a>Lc!-EwD:,_.🌟☺–NAmM-8🌟1V&Cp\n",
            "dàabO9t;àd¡w🔐]/G🔒Bd]u(>xBp2E(Eg8\taN️sJG🙁7-🤝,Ma{CEg🙁Cp`vso\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# ✅ Define the Bigram Language Model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # A lookup table that maps token indices to vocab-sized logits for next token\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx: (B, T) input indices\n",
        "        # targets: (B, T) expected next-token indices\n",
        "\n",
        "        # Embed the tokens (output: logits of shape [B, T, vocab_size])\n",
        "        logits = self.token_embedding_table(idx)  # (B, T, C)\n",
        "\n",
        "        # If we're training (targets are provided), compute the loss\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)        # Flatten (B, T, C) -> (B*T, C)\n",
        "            targets = targets.view(B * T)         # Flatten targets: (B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)               # (B, T, C)\n",
        "            logits = logits[:, -1, :]           # focus on last time step (B, C)\n",
        "\n",
        "            # logits = logits[:, -1, :]\n",
        "                        # └─┬─┘ └─┬─┘ └─┬─┘\n",
        "                          # │     │     └── all class-scores\n",
        "                          # │     └──────── last time-step (t = T-1)\n",
        "                          # └────────────── all batch entries\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)   # convert to probabilities (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # sample (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # append to sequence (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# ✅ Instantiate and test the model\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(\"Logits shape:\", logits.shape)  # [B*T, vocab_size]\n",
        "print(\"Loss:\", loss)                  # Cross-entropy loss\n",
        "\n",
        "# ✅ Try generating some text from the model\n",
        "sample = m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)\n",
        "print(\"\\nGenerated text (raw tokens):\", sample[0].tolist())\n",
        "print(\"\\nGenerated text (decoded):\")\n",
        "print(decode(sample[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQpbK47Sv96Q"
      },
      "source": [
        "#### Train the Model\n",
        "\n",
        "Optimize the model using a basic training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Build** your Bigram model and AdamW optimizer.\n",
        "2. **Repeat 100×:**\n",
        "\n",
        "   * **Grab** a random batch (`xb`, `yb`).\n",
        "   * **Forward** through the model to get `loss`.\n",
        "   * **Backprop** to optimise the logits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4Q5Hn7gwAqn",
        "outputId": "1447a7af-93f5-45d2-f629-bb51c92f1497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.332954406738281\n",
            "5.358665943145752\n",
            "5.151556968688965\n",
            "5.235673904418945\n",
            "5.46928071975708\n",
            "5.0979108810424805\n",
            "5.282344341278076\n",
            "5.01500129699707\n",
            "5.568490505218506\n",
            "5.367368221282959\n",
            "5.1276350021362305\n",
            "5.375743865966797\n",
            "5.245172023773193\n",
            "5.231736660003662\n",
            "5.352851390838623\n",
            "5.219082832336426\n",
            "5.791552543640137\n",
            "4.828013896942139\n",
            "5.439228534698486\n",
            "5.397055625915527\n",
            "4.983537197113037\n",
            "5.428558826446533\n",
            "5.535346031188965\n",
            "5.413728713989258\n",
            "5.238778591156006\n",
            "5.203005790710449\n",
            "5.35411262512207\n",
            "5.070978164672852\n",
            "5.397096157073975\n",
            "5.209497451782227\n",
            "4.900765419006348\n",
            "5.011595726013184\n",
            "5.300110816955566\n",
            "5.278593063354492\n",
            "5.2811760902404785\n",
            "5.214981555938721\n",
            "5.351513862609863\n",
            "5.1683735847473145\n",
            "5.572271347045898\n",
            "5.3149189949035645\n",
            "5.360107421875\n",
            "5.484967231750488\n",
            "5.286406517028809\n",
            "5.2235894203186035\n",
            "5.5175957679748535\n",
            "5.22564172744751\n",
            "5.440962791442871\n",
            "5.232370853424072\n",
            "4.816390514373779\n",
            "5.533224105834961\n",
            "5.294939994812012\n",
            "5.235952854156494\n",
            "5.159475326538086\n",
            "5.338944435119629\n",
            "5.122669219970703\n",
            "5.318301200866699\n",
            "5.039262771606445\n",
            "5.0875654220581055\n",
            "5.125899314880371\n",
            "5.169435501098633\n",
            "5.076278209686279\n",
            "5.198418617248535\n",
            "5.216441631317139\n",
            "5.234527587890625\n",
            "5.32163143157959\n",
            "5.1013031005859375\n",
            "5.305529594421387\n",
            "5.2944464683532715\n",
            "5.256989479064941\n",
            "5.013661861419678\n",
            "5.149155616760254\n",
            "5.216480255126953\n",
            "5.260592460632324\n",
            "5.1395158767700195\n",
            "5.127189636230469\n",
            "5.429296016693115\n",
            "5.170031547546387\n",
            "4.989600658416748\n",
            "5.036764621734619\n",
            "4.954488277435303\n",
            "5.679506778717041\n",
            "5.295421600341797\n",
            "4.995811462402344\n",
            "5.218926906585693\n",
            "5.440976619720459\n",
            "5.138448715209961\n",
            "4.866363048553467\n",
            "5.155867576599121\n",
            "5.018071174621582\n",
            "5.328635215759277\n",
            "5.129188537597656\n",
            "5.196531772613525\n",
            "5.242077350616455\n",
            "4.992900371551514\n",
            "5.092016696929932\n",
            "5.160769462585449\n",
            "5.282329082489014\n",
            "5.406414985656738\n",
            "5.574610710144043\n",
            "5.242220878601074\n"
          ]
        }
      ],
      "source": [
        "m = BigramLanguageModel(vocab_size)\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "for steps in range(100):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdOvbH6ZwPxP",
        "outputId": "eb3ae1d9-f471-45a3-8edc-d77ea89f1845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t🔐vym-U{☺:👍Dm[)_—ED\t️M;W+–n🤗RQ😊 `lJO,v¡p🤝d🙁k'&zwUq5🙁👍[🔐;B$k🙏1🔐8p.90{👍4k,\"(e{EG5.+nb?k2\"Ho é¡yà💪V.A1+K\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHxskkLLxR1Y"
      },
      "source": [
        "#### Issues with Simple Bigram Model\n",
        "\n",
        "1. **Zero Context Beyond One Token**\n",
        "\n",
        "2. **No Positional or Segment Embeddings**\n",
        "   * Every character treated identically, regardless of its position in the sequence.\n",
        "   * Model has no sense of “this is the start,” “this is the end,” or token order beyond one step.\n",
        "\n",
        "3. **Tiny Context Window (`block_size`)**\n",
        "   * We used very small chunks (e.g. 8 tokens) for demonstration → which severely limits what the model can memorize or generalize.\n",
        "\n",
        "4. **Character‐Level Tokenization Only**\n",
        "   * Single characters as tokens → extremely long sequences and slow convergence.\n",
        "\n",
        "5. **No Attention or Deep Layers**\n",
        "\n",
        "6. **No Loss Smoothing or Metrics**\n",
        "\n",
        "   * We printed raw loss per step → extremely noisy signal due to random mini‐batches, without averaging or exponential smoothing.\n",
        "\n",
        "---\n",
        "\n",
        "#### What’s Next: Building a Better Bigram Language Model\n",
        "\n",
        "1. **Longer Context Windows**\n",
        "   * Increase `block_size` to dozens or hundreds of tokens so the model sees more history.\n",
        "\n",
        "2. **Subword Tokenization (BPE / WordPiece)**\n",
        "\n",
        "3. **Positional & Segment Embeddings**\n",
        "\n",
        "4. **Self-Attention**\n",
        "   * Introduce multi-head attention layers\n",
        "\n",
        "5. **Loss Smoothing**\n",
        "\n",
        "In the next video, we’ll upgrade our simple bigram into a much better bigram llanguage model before we build our first transformer model.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
