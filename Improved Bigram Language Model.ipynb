{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbsKdAQXxJyL"
      },
      "source": [
        "## üß† Building on the Bigram Language model\n",
        "\n",
        "**Improving the Bigram LM**\n",
        "*(Still conditions only on token *t*)*\n",
        "\n",
        "1. **Longer Context Windows**\n",
        "\n",
        "   * Increase `block_size` from 1 to, e.g., 128.\n",
        "   * *Why?*: You can process longer sequences in each batch‚Äîbut the prediction at *t+1* still only uses the embedding at *t*.\n",
        "\n",
        "2. **Subword Tokenization (BPE/WordPiece)**\n",
        "\n",
        "   * Replace characters with subword units (e.g. ‚Äúrefund‚Äù ‚Üí one token).\n",
        "   * *Why?*: Sequences become semantically richer and shorter‚Äîyet you still map one token to the next via a single lookup.\n",
        "\n",
        "3. **Token + Positional (¬± Segment) Embeddings**\n",
        "\n",
        "   * *Why?*: Adds learned semantic vectors and absolute order (or speaker) info‚Äîbut if you collapse only x\\[t] to predict the next token, the model remains a bigram.\n",
        "\n",
        "---\n",
        "\n",
        "**Minimal Transformer LM**\n",
        "*(Now conditions on all tokens 0‚Ä¶t)*\n",
        "\n",
        "4. **Self-Attention + Causal Masking**\n",
        "\n",
        "   * *Why?*: Each position *t* now ‚Äúlooks at‚Äù tokens 0‚Ä¶*t*‚Äì1 when forming its representation.  That full-history dependency is what **breaks** the bigram limitation and makes your model a **decoder-only Transformer**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIHQYQ-cy84F"
      },
      "source": [
        "####  **Load from HugginFace as Pandas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8mgt2pQzBWO",
        "outputId": "98b13c96-46f0-473f-d5b5-57d71a76e2df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hf://datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rG8mVgxRzFXv",
        "outputId": "c7420bc1-cb72-448c-8fa9-20c4e7569f2c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'customer_support_data.txt'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairs = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "  prompt =row['instruction'].strip()\n",
        "  reply = row['response'].strip()\n",
        "\n",
        "  if prompt and reply:\n",
        "    text_pair = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{reply}\\n\\n\"\n",
        "    pairs.append(text_pair)\n",
        "\n",
        "\n",
        "full_text = \"\".join(pairs)\n",
        "\n",
        "\n",
        "output_path = 'customer_support_data.txt'\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(full_text)\n",
        "\n",
        "text = open('customer_support_data.txt', 'r').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQzxqYTy1If_"
      },
      "source": [
        "### ‚úèÔ∏è Step 2: Create Tokenizer - Subword (BPE) Level (Encoder/Decoder)\n",
        "\n",
        "Instead of a character-level vocabulary, we‚Äôll use a GPT-2‚Äìstyle BPE tokenizer to break text into meaningful subword units. This produces shorter, semantically richer sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHpU_dPO1Ps2",
        "outputId": "8ef3ac92-e7b7-4116-e957-1c225494313d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: LLM\n",
            "Encoded: [43, 43, 44]\n",
            "Decoded: LLM\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "# 1Ô∏è‚É£ Initialize the BPE encoder (GPT-2 vocab)\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "# 2Ô∏è‚É£ Replace character-level stoi/itos with BPE encode/decode\n",
        "encode = lambda s: enc.encode(s)        # str ‚Üí list[int] (subword IDs)\n",
        "decode = lambda ids: enc.decode(ids)    # list[int] ‚Üí str\n",
        "\n",
        "# 3Ô∏è‚É£ Example usage\n",
        "sample_text = \"LLM\"\n",
        "encoded_ids = encode(sample_text)\n",
        "decoded_text = decode(encoded_ids)\n",
        "\n",
        "print(\"Original text: \", sample_text)\n",
        "print(\"Encoded IDs:   \", encoded_ids)\n",
        "print(\"Decoded text:  \", decoded_text)\n",
        "\n",
        "# 4Ô∏è‚É£ Integrate into your dataset pipeline\n",
        "full_text = open('customer_support_data.txt').read()\n",
        "data_ids = encode(full_text)                  # tokenize entire corpus\n",
        "data     = torch.tensor(data_ids, dtype=torch.long)\n",
        "vocab_size = enc.n_vocab\n",
        "print(\"Vocab size:\", vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzNa4pEo1Ynm"
      },
      "source": [
        "### ‚úèÔ∏è Step 4: Encode the Entire Dataset\n",
        "\n",
        "Now that we have defined our **encoder** and **decoder**, we can use them to convert the **entire text dataset** into a sequence of integers. This forms the actual training data that we‚Äôll feed into the language model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0m9no421m4U",
        "outputId": "a49d273f-eb5f-4f2d-f41d-a1140125f744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: torch.Size([19240191])\n",
            "Data type: torch.int64\n",
            "First 1000 tokens:\n",
            " tensor([ 5,  5,  5,  2, 40, 75, 80, 81, 79, 82, 64, 81, 70, 76, 75, 27,  1, 78,\n",
            "        82, 66, 80, 81, 70, 76, 75,  2, 62, 63, 76, 82, 81,  2, 64, 62, 75, 64,\n",
            "        66, 73, 73, 70, 75, 68,  2, 76, 79, 65, 66, 79,  2, 88, 88, 46, 79, 65,\n",
            "        66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89,  1,  1,  5,  5,  5,  2, 49,\n",
            "        66, 80, 77, 76, 75, 80, 66, 27,  1, 40,  8, 83, 66,  2, 82, 75, 65, 66,\n",
            "        79, 80, 81, 76, 76, 65,  2, 86, 76, 82,  2, 69, 62, 83, 66,  2, 62,  2,\n",
            "        78, 82, 66, 80, 81, 70, 76, 75,  2, 79, 66, 68, 62, 79, 65, 70, 75, 68,\n",
            "         2, 64, 62, 75, 64, 66, 73, 70, 75, 68,  2, 76, 79, 65, 66, 79,  2, 88,\n",
            "        88, 46, 79, 65, 66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89, 13,  2, 62,\n",
            "        75, 65,  2, 40,  8, 74,  2, 69, 66, 79, 66,  2, 81, 76,  2, 77, 79, 76,\n",
            "        83, 70, 65, 66,  2, 86, 76, 82,  2, 84, 70, 81, 69,  2, 81, 69, 66,  2,\n",
            "        70, 75, 67, 76, 79, 74, 62, 81, 70, 76, 75,  2, 86, 76, 82,  2, 75, 66,\n",
            "        66, 65, 15,  2, 47, 73, 66, 62, 80, 66,  2, 68, 76,  2, 62, 69, 66, 62,\n",
            "        65,  2, 62, 75, 65,  2, 62, 80, 72,  2, 86, 76, 82, 79,  2, 78, 82, 66,\n",
            "        80, 81, 70, 76, 75, 13,  2, 62, 75, 65,  2, 40,  8, 73, 73,  2, 65, 76,\n",
            "         2, 74, 86,  2, 63, 66, 80, 81,  2, 81, 76,  2, 62, 80, 80, 70, 80, 81,\n",
            "         2, 86, 76, 82, 15,  1,  1,  5,  5,  5,  2, 40, 75, 80, 81, 79, 82, 64,\n",
            "        81, 70, 76, 75, 27,  1, 70,  2, 69, 62, 83, 66,  2, 62,  2, 78, 82, 66,\n",
            "        80, 81, 70, 76, 75,  2, 62, 63, 76, 82, 81,  2, 64, 62, 75, 64, 66, 73,\n",
            "        73, 70, 75, 68,  2, 76, 76, 79, 65, 66, 79,  2, 88, 88, 46, 79, 65, 66,\n",
            "        79,  2, 45, 82, 74, 63, 66, 79, 89, 89,  1,  1,  5,  5,  5,  2, 49, 66,\n",
            "        80, 77, 76, 75, 80, 66, 27,  1, 40,  8, 83, 66,  2, 63, 66, 66, 75,  2,\n",
            "        70, 75, 67, 76, 79, 74, 66, 65,  2, 81, 69, 62, 81,  2, 86, 76, 82,  2,\n",
            "        69, 62, 83, 66,  2, 62,  2, 78, 82, 66, 80, 81, 70, 76, 75,  2, 62, 63,\n",
            "        76, 82, 81,  2, 64, 62, 75, 64, 66, 73, 70, 75, 68,  2, 76, 79, 65, 66,\n",
            "        79,  2, 88, 88, 46, 79, 65, 66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89,\n",
            "        15,  2, 40,  8, 74,  2, 69, 66, 79, 66,  2, 81, 76,  2, 62, 80, 80, 70,\n",
            "        80, 81,  2, 86, 76, 82,  3,  2, 47, 73, 66, 62, 80, 66,  2, 68, 76,  2,\n",
            "        62, 69, 66, 62, 65,  2, 62, 75, 65,  2, 73, 66, 81,  2, 74, 66,  2, 72,\n",
            "        75, 76, 84,  2, 84, 69, 62, 81,  2, 80, 77, 66, 64, 70, 67, 70, 64,  2,\n",
            "        78, 82, 66, 80, 81, 70, 76, 75,  2, 86, 76, 82,  2, 69, 62, 83, 66, 13,\n",
            "         2, 62, 75, 65,  2, 40,  8, 73, 73,  2, 77, 79, 76, 83, 70, 65, 66,  2,\n",
            "        86, 76, 82,  2, 84, 70, 81, 69,  2, 62, 73, 73,  2, 81, 69, 66,  2, 70,\n",
            "        75, 67, 76, 79, 74, 62, 81, 70, 76, 75,  2, 62, 75, 65,  2, 68, 82, 70,\n",
            "        65, 62, 75, 64, 66,  2, 86, 76, 82,  2, 75, 66, 66, 65, 15,  2, 56, 76,\n",
            "        82, 79,  2, 80, 62, 81, 70, 80, 67, 62, 64, 81, 70, 76, 75,  2, 70, 80,\n",
            "         2, 74, 86,  2, 81, 76, 77,  2, 77, 79, 70, 76, 79, 70, 81, 86, 15,  1,\n",
            "         1,  5,  5,  5,  2, 40, 75, 80, 81, 79, 82, 64, 81, 70, 76, 75, 27,  1,\n",
            "        70,  2, 75, 66, 66, 65,  2, 69, 66, 73, 77,  2, 64, 62, 75, 64, 66, 73,\n",
            "        73, 70, 75, 68,  2, 77, 82, 64, 69, 62, 80, 66,  2, 88, 88, 46, 79, 65,\n",
            "        66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89,  1,  1,  5,  5,  5,  2, 49,\n",
            "        66, 80, 77, 76, 75, 80, 66, 27,  1, 40,  2, 64, 62, 75,  2, 80, 66, 75,\n",
            "        80, 66,  2, 81, 69, 62, 81,  2, 86, 76, 82,  8, 79, 66,  2, 80, 66, 66,\n",
            "        72, 70, 75, 68,  2, 62, 80, 80, 70, 80, 81, 62, 75, 64, 66,  2, 84, 70,\n",
            "        81, 69,  2, 64, 62, 75, 64, 66, 73, 70, 75, 68,  2, 86, 76, 82, 79,  2,\n",
            "        77, 82, 79, 64, 69, 62, 80, 66,  2, 84, 70, 81, 69,  2, 81, 69, 66,  2,\n",
            "        77, 82, 79, 64, 69, 62, 80, 66,  2, 75, 82, 74, 63, 66, 79,  2, 88, 88,\n",
            "        46, 79, 65, 66, 79,  2, 45, 82, 74, 63, 66, 79, 89, 89, 15,  2, 40,  2,\n",
            "        62, 77, 76, 73, 76, 68, 70, 87, 66,  2, 67, 76, 79,  2, 62, 75, 86,  2,\n",
            "        70, 75, 64, 76, 75, 83, 66, 75, 70, 66, 75, 64, 66,  2, 64, 62, 82, 80,\n",
            "        66, 65, 13,  2, 62, 75, 65,  2, 40,  8, 74,  2, 69, 66, 79, 66,  2, 81,\n",
            "        76,  2, 68, 82, 70, 65, 66,  2, 86, 76, 82,  2, 81, 69, 79, 76, 82, 68,\n",
            "        69,  2, 81, 69, 66,  2, 77, 79, 76, 64, 66, 80, 80, 15,  1,  1, 51, 76,\n",
            "         2, 64, 62, 75, 64, 66, 73,  2, 86, 76, 82, 79,  2, 77, 82, 79, 64, 69,\n",
            "        62, 80, 66, 13,  2, 77, 73, 66, 62, 80, 66,  2, 67, 76, 73, 73, 76, 84,\n",
            "         2, 81, 69, 66, 80, 66,  2, 80, 81, 66])\n"
          ]
        }
      ],
      "source": [
        "import torch  # We use PyTorch: https://pytorch.org\n",
        "\n",
        "# Encode the entire text using our character-level encoder\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Print tensor information\n",
        "print(\"Data shape:\", data.shape)\n",
        "print(\"Data type:\", data.dtype)\n",
        "\n",
        "# Preview first 1000 encoded tokens\n",
        "print(\"First 1000 tokens:\\n\", data[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V3-evFO1rKk"
      },
      "source": [
        "### üåê Step 5: Train/Test Split\n",
        "\n",
        "Split data into training (90%) and validation (10%) sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rHIN95wr1r4e"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uZt883w15yQ"
      },
      "source": [
        "### üöó Step 6: Create Data Batches\n",
        "\n",
        "So far, we've seen how a **single sequence (or chunk)** of text helps a model learn by predicting the **next character** at every position. But training on one sequence at a time is inefficient. To train faster and learn more general patterns, we move to **batching**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md9xqP2s2fi0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4   # How many sequences to process in parallel\n",
        "block_size = 128   # Length of each sequence (context window)\n",
        "\n",
        "def get_batch(split):\n",
        "    \"\"\"\n",
        "    Samples a mini-batch of input (x) and target (y) sequences from the dataset.\n",
        "\n",
        "    Args:\n",
        "        split (str): One of 'train' or 'val' to choose the dataset split.\n",
        "\n",
        "    Returns:\n",
        "        x (torch.Tensor): Input sequences of shape (batch_size, block_size)\n",
        "        y (torch.Tensor): Target sequences of shape (batch_size, block_size)\n",
        "                          Each y[i, t] is the next character after x[i, t]\n",
        "    \"\"\"\n",
        "    assert split in ['train', 'val'], \"split must be 'train' or 'val'\"\n",
        "\n",
        "    data_source = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Randomly sample starting indices for each sequence\n",
        "    start_indices = torch.randint(0, len(data_source) - block_size, (batch_size,))\n",
        "\n",
        "    # Build input and target tensors using slicing\n",
        "    x = torch.stack([data_source[i:i + block_size] for i in start_indices])\n",
        "    y = torch.stack([data_source[i + 1:i + block_size + 1] for i in start_indices])\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scJ41ZV02kca",
        "outputId": "391a8ea4-4403-4123-dc52-8105bce5a331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßÆ Input batch shape: torch.Size([4, 8])\n",
            "üßÆ Target batch shape: torch.Size([4, 8])\n",
            "\n",
            "üßæ Inputs (xb):\n",
            "tensor([[62, 81,  2, 86, 76, 82,  8, 79],\n",
            "        [79, 66, 65, 66, 75, 81, 70, 62],\n",
            "        [ 2, 82, 75, 65, 66, 79, 80, 81],\n",
            "        [84, 70, 81, 69,  2, 81, 69, 66]])\n",
            "\n",
            "üéØ Targets (yb):\n",
            "tensor([[81,  2, 86, 76, 82,  8, 79, 66],\n",
            "        [66, 65, 66, 75, 81, 70, 62, 73],\n",
            "        [82, 75, 65, 66, 79, 80, 81, 62],\n",
            "        [70, 81, 69,  2, 81, 69, 66,  2]])\n"
          ]
        }
      ],
      "source": [
        "# Generate a training batch\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "# Inspect the shape of the input and target tensors\n",
        "print(\"üßÆ Input batch shape:\", xb.shape)   # Expected: (4, 8)\n",
        "print(\"üßÆ Target batch shape:\", yb.shape) # Expected: (4, 8)\n",
        "\n",
        "# View actual data\n",
        "print(\"\\nüßæ Inputs (xb):\")\n",
        "print(xb)\n",
        "\n",
        "print(\"\\nüéØ Targets (yb):\")\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Bigram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# ‚úÖ Define the Bigram LM with Token + Positional Embeddings\n",
        "class BigramWithPos(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, block_size):\n",
        "        super().__init__()\n",
        "        # token & positional embeddings\n",
        "        self.tok_emb = nn.Embedding(vocab_size, emb_size)\n",
        "        self.pos_emb = nn.Embedding(block_size, emb_size)\n",
        "        # final head to project back to vocab logits\n",
        "        self.lm_head = nn.Linear(emb_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # embed tokens and positions\n",
        "        tok = self.tok_emb(idx)                                        # (B, T, E)\n",
        "        pos = self.pos_emb(torch.arange(T, device=idx.device))[None]   # (1, T, E)\n",
        "        x   = tok + pos                                                 # (B, T, E)\n",
        "\n",
        "        # compute logits\n",
        "        logits = self.lm_head(x)                                        # (B, T, V)\n",
        "\n",
        "        # compute loss if targets are provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(B * T, -1),\n",
        "                targets.view(B * T)\n",
        "            )\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # autoregressively sample from the model\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self(idx)               # (B, T, V)\n",
        "            probs      = F.softmax(logits[:, -1, :], dim=-1)  # (B, V)\n",
        "            next_id    = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            idx        = torch.cat([idx, next_id], dim=1)      # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "# ‚îÄ‚îÄ Instantiate & Test ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "block_size = 128\n",
        "emb_size   = 128\n",
        "\n",
        "model = BigramWithPos(vocab_size, emb_size, block_size)\n",
        "\n",
        "# test forward and loss\n",
        "logits, loss = model(xb, yb)\n",
        "print(\"Logits shape:\", logits.shape)  # (B, T, V)\n",
        "print(\"Loss:\", loss.item())\n",
        "\n",
        "# generate sample text\n",
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "sample_ids = model.generate(context, max_new_tokens=100)[0].tolist()\n",
        "print(\"\\nGenerated text:\\n\", decode(sample_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ‚îÄ‚îÄ Hyperparameters ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "batch_size    = 32\n",
        "block_size    = 128\n",
        "emb_size      = 128\n",
        "learning_rate = 3e-4\n",
        "num_steps     = 50\n",
        "ema_alpha     = 0.99\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "ema_loss = None\n",
        "\n",
        "for step in range(1, num_steps + 1):\n",
        "    xb, yb = get_batch('train')              # (B, T)\n",
        "    _, loss = model(xb, targets=yb)          # forward + loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    raw = loss.item()\n",
        "    ema_loss = raw if ema_loss is None else ema_alpha * ema_loss + (1 - ema_alpha) * raw\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step:4d} | raw loss = {raw:.4f} | ema loss = {ema_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inference with the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    xb_val, yb_val = get_batch('val')\n",
        "    _, val_loss = model(xb_val, targets=yb_val)\n",
        "print(f\"\\nValidation loss: {val_loss.item():.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# we won‚Äôt need gradients\n",
        "with torch.no_grad():\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=next(model.parameters()).device)\n",
        "    \n",
        "    out_ids = model.generate(context, max_new_tokens=100)[0].tolist()\n",
        "    \n",
        "print(decode(out_ids))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
