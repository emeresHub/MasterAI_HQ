{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbsKdAQXxJyL"
      },
      "source": [
        "## 🧠 How attenton works "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UsadW6354X5"
      },
      "source": [
        "In our **bigram model**, each token predicted the next without any real understanding of what came before. It had **no context**, just a lookup table. That’s very limited.\n",
        "\n",
        "Now we want to go **beyond bigrams** and allow tokens to **\"look back\"** and summarize what’s happened so far — this is where attention begins and the beginning of transformer models\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 The Core Idea\n",
        "\n",
        "Imagine a sequence of 8 tokens like:\n",
        "\n",
        "```\n",
        "\n",
        "\\[Token₁, Token₂, Token₃, ..., Token₈]\n",
        "\n",
        "````\n",
        "\n",
        "- The **5th token** should ideally make decisions based on tokens `1, 2, 3, 4`.\n",
        "- It should **not** \"see the future\" (tokens `6, 7, 8`), since we're generating left-to-right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Naive-based Averaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_lOSW346HBS",
        "outputId": "deec7477-17d3-4aec-b79c-b0d82dc34bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input x[0]:\n",
            "tensor([[-0.8345,  0.5978],\n",
            "        [-0.0514, -0.0646],\n",
            "        [-0.4970,  0.4658],\n",
            "        [-0.2573, -1.0673],\n",
            "        [ 2.0089, -0.5370],\n",
            "        [ 0.2228,  0.6971],\n",
            "        [-1.4267,  0.9059],\n",
            "        [ 0.1446,  0.2280]])\n",
            "\n",
            "Naive Averaged xbow[0]:\n",
            "tensor([[-0.8345,  0.5978],\n",
            "        [-0.4429,  0.2666],\n",
            "        [-0.4610,  0.3330],\n",
            "        [-0.4100, -0.0171],\n",
            "        [ 0.0738, -0.1210],\n",
            "        [ 0.0986,  0.0153],\n",
            "        [-0.1193,  0.1425],\n",
            "        [-0.0863,  0.1532]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "B, T, C = 4, 8, 2  # Batch size, Time steps (sequence length), Channels (embedding dim)\n",
        "x = torch.randn(B, T, C)  # Random input (think of this like token embeddings)\n",
        "\n",
        "# Allocate output tensor\n",
        "xbow = torch.zeros(B, T, C)\n",
        "\n",
        "# For each time step, average over all past and current tokens\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xbow[b, t] = torch.mean(x[b, :t+1], dim=0)  # Mean of tokens up to t\n",
        "\n",
        "# Print the original input for batch 0\n",
        "print(\"Input x[0]:\")\n",
        "print(x[0])\n",
        "\n",
        "# Print the contextualized embeddings (averaged tokens)\n",
        "print(\"\\nNaive Averaged xbow[0]:\")\n",
        "print(xbow[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwWm79rF7ZRn"
      },
      "source": [
        "### 🔎 What’s Actually Happening Naive-based Average?\n",
        "\n",
        "1. **Original Embedding**  \n",
        "   - `x[0][4]` is the raw embedding for **token 5** (index 4) in sequence 0.  \n",
        "\n",
        "2. **Averaged Embedding**  \n",
        "   - `xbow[0][4]` = **mean** of `x[0][0:5]` → the average of embeddings for tokens 1–5.  \n",
        "   - This gives a **smoothed, context-aware** vector that “remembers” everything seen so far.\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Step-by-Step Visual\n",
        "\n",
        "Suppose your sequence embeddings look like this:\n",
        "\n",
        "| Time step (t) | 0  | 1  | 2  | 3  | 4  | 5  | 6  | 7  |\n",
        "|--------------:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
        "| Token embedding | T₀ | T₁ | T₂ | T₃ | T₄ | T₅ | T₆ | T₇ |\n",
        "\n",
        "Then for each position _t_:\n",
        "\n",
        "- **xbow[0]** = mean([ T₀ ])  \n",
        "- **xbow[1]** = mean([ T₀, T₁ ])  \n",
        "- **xbow[2]** = mean([ T₀, T₁, T₂ ])  \n",
        "- …  \n",
        "- **xbow[7]** = mean([ T₀, T₁, T₂, T₃, T₄, T₅, T₆, T₇ ])  \n",
        "\n",
        "In other words, **every xbow[t] blends all tokens from 0…t** into a single vector.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ Key Limitations\n",
        "\n",
        "- **Order lost:**  T₀ + T₁ = T₁ + T₀  \n",
        "- **Uniform weights:** All past tokens count equally  \n",
        "- **Inefficient loops:** O(T²) Python loops  \n",
        "- **Blurred details:** Sharp patterns get diluted  \n",
        "\n",
        "> 👉 Despite these drawbacks, averaging introduces the fundamental idea that **each token should incorporate its history**—a stepping stone toward full self-attention.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPQAdjm3_0bE"
      },
      "source": [
        "## 2.Efficient Averaging Using Matrix Multiplication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRt8Ce9mAOpK",
        "outputId": "8a8a1542-ada1-4107-dc2d-6d8952b5b36e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a =\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "b =\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c =\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Example dimensions\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)  # (B, T, C) token embeddings\n",
        "\n",
        "# 1️⃣ Build the causal mask (T×T)\n",
        "a = torch.tril(torch.ones(T, T))           # shape = (T, T)\n",
        "# 2️⃣ Normalize each row to sum to 1\n",
        "a = a / a.sum(dim=1, keepdim=True)         # still (T, T)\n",
        "\n",
        "# 3️⃣ Vectorized averaging: apply mask to x\n",
        "#    (T×T) @ (B×T×C) → (B×T×C), broadcasting over batch dim\n",
        "xbow_vec = a @ x                           # shape = (B, T, C)\n",
        "\n",
        "# 4️⃣ Inspect results for batch 0\n",
        "print(\"Original x[0]:\")\n",
        "print(x[0])\n",
        "\n",
        "print(\"\\nVectorized Averaged xbow_vec[0]:\")\n",
        "print(xbow_vec[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🔎 What’s Actually Happening with Matrix Averaging?\n",
        "\n",
        "1. **Causal Mask Matrix**  \n",
        "   - We build a lower-triangular matrix **a** of shape (T, T), where  \n",
        "     ```python\n",
        "     a = torch.tril(torch.ones(T, T))\n",
        "     ```  \n",
        "   - This mask has 1’s for positions ≤ t and 0’s for future positions > t.\n",
        "\n",
        "2. **Row Normalization**  \n",
        "   - We normalize each row so it sums to 1:  \n",
        "     ```python\n",
        "     a = a / a.sum(1, keepdim=True)\n",
        "     ```  \n",
        "   - Now **a[t]** is a uniform averaging distribution over tokens 0…t.\n",
        "\n",
        "3. **Vectorized Averaging**  \n",
        "   - Multiply the mask by your embedding tensor `x` (shape `(B, T, C)`):  \n",
        "     ```python\n",
        "     xbow = a @ x  # result shape (B, T, C)\n",
        "     ```  \n",
        "   - For each position _t_, `xbow[:, t, :]` equals the mean of `x[:, 0:t+1, :]` across that row of **a**.\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Step-by-Step Visual (T=3 example)\n",
        "\n",
        "1. **Build & Normalize**  \n",
        "   ```python\n",
        "   a = torch.tril(torch.ones(3, 3))\n",
        "   # a = [[1,0,0],\n",
        "   #      [1,1,0],\n",
        "   #      [1,1,1]]\n",
        "\n",
        "   a = a / a.sum(1, keepdim=True)\n",
        "   # a = [[1.0, 0.0, 0.0],\n",
        "   #      [0.5, 0.5, 0.0],\n",
        "   #      [0.33,0.33,0.33]]\n",
        "\n",
        "\n",
        "👉 This vectorized trick is the exact same as the avergaing method seen above and has same limitation but it removes Python loops and makes it more efficient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mERnVAj8BQNq"
      },
      "source": [
        "#### 🧠 Apply to All Batches Efficiently\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WmIK36cBU0H",
        "outputId": "9a5f2e87-1bc1-4a54-bd42-8d6a3902c5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original x[0]:\n",
            "tensor([[-0.8345,  0.5978],\n",
            "        [-0.0514, -0.0646],\n",
            "        [-0.4970,  0.4658],\n",
            "        [-0.2573, -1.0673],\n",
            "        [ 2.0089, -0.5370],\n",
            "        [ 0.2228,  0.6971],\n",
            "        [-1.4267,  0.9059],\n",
            "        [ 0.1446,  0.2280]])\n",
            "\n",
            "Naive Averaged xbow[0]:\n",
            "tensor([[-0.8345,  0.5978],\n",
            "        [-0.4429,  0.2666],\n",
            "        [-0.4610,  0.3330],\n",
            "        [-0.4100, -0.0171],\n",
            "        [ 0.0738, -0.1210],\n",
            "        [ 0.0986,  0.0153],\n",
            "        [-0.1193,  0.1425],\n",
            "        [-0.0863,  0.1532]])\n",
            "\n",
            "Efficient Averaged xbow2[0]:\n",
            "tensor([[-0.8345,  0.5978],\n",
            "        [-0.4429,  0.2666],\n",
            "        [-0.4610,  0.3330],\n",
            "        [-0.4100, -0.0171],\n",
            "        [ 0.0738, -0.1210],\n",
            "        [ 0.0986,  0.0153],\n",
            "        [-0.1193,  0.1425],\n",
            "        [-0.0863,  0.1532]])\n",
            "\n",
            "Do they match?\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 2\n",
        "\n",
        "# Using same x as in Naive Approach\n",
        "\n",
        "# Naive for-loop version\n",
        "xbow = torch.zeros(B, T, C)\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xbow[b, t] = torch.mean(x[b, :t+1], dim=0)\n",
        "\n",
        "# Efficient matrix version\n",
        "wei = torch.tril(torch.ones(T, T))          # Causal mask\n",
        "wei = wei / wei.sum(1, keepdim=True)        # Normalize each row\n",
        "xbow2 = wei @ x                             # Broadcast over batch\n",
        "\n",
        "# Compare outputs\n",
        "print(\"Original x[0]:\")\n",
        "print(x[0])\n",
        "\n",
        "print(\"\\nNaive Averaged xbow[0]:\")\n",
        "print(xbow[0])\n",
        "\n",
        "print(\"\\nEfficient Averaged xbow2[0]:\")\n",
        "print(xbow2[0])\n",
        "\n",
        "# Confirm they match\n",
        "print(\"\\nDo they match?\")\n",
        "print(torch.allclose(xbow, xbow2, rtol=1e-4, atol=1e-6))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51uDmWLjDpk4"
      },
      "source": [
        "## 🧠 Final Version: Weighted Averaging with Softmax — the Birth of Attention\n",
        "\n",
        "We’ve seen how to compute averages using a **triangular matrix**, where each token only sees previous tokens.\n",
        "\n",
        "But what if we want to assign **different importance** to each past token — instead of giving equal weight?\n",
        "\n",
        "That’s what self-attention does! It uses **softmax** to assign **learnable, normalized weights** to each token’s past.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔶 Step 1: Create a Causal Mask\n",
        "\n",
        "We want tokens to only attend to the **past**, so we use a lower-triangular matrix again:\n",
        "\n",
        "This is a **causal mask** — it ensures that token `t` can only \"see\" tokens `0...t`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQsAUyCGD3jV",
        "outputId": "a4cd3b58-4fad-4791-d700-bd11f5d63c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "T = 4  # Sequence length\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "\n",
        "print(tril)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYT1D5NyEAyT"
      },
      "source": [
        "### 🔶 Step 2: Mask Out the Future with `-inf`\n",
        "\n",
        "➡️ This fills all **future positions** with `-inf`. The current and past stay `0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0sn-pghEEF3",
        "outputId": "11bd9a7e-34bd-489d-8972-89b820c2bd9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf],\n",
            "        [0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "wei = torch.zeros(T, T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "\n",
        "print(wei)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt6j0Jd3EP3j"
      },
      "source": [
        "### 🔶 Step 3: Apply Softmax\n",
        "\n",
        "💡 **Softmax** turns each row into a **probability distribution**:\n",
        "\n",
        "* All weights on each row add up to 1\n",
        "* More recent tokens get higher weight by default (if logits were all equal)\n",
        "\n",
        "> **Bottom line:** Softmax turns arbitrary scores into a “soft pick” over past tokens—equal if scores are equal, or biased toward the most relevant ones when scores differ.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjKWWcgxEZWl",
        "outputId": "a270b8f0-1393-4981-c650-1b491c3532b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "print(wei)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCiLqW7REfm8"
      },
      "source": [
        "### 🔁 What's Happening?\n",
        "\n",
        "Let’s say we have 4 tokens: `T₀, T₁, T₂, T₃`\n",
        "\n",
        "The attention weights become:\n",
        "\n",
        "| Token | Weighted average of          |\n",
        "| ----- | ---------------------------- |\n",
        "| `T₀`  | `T₀` only (1.0)              |\n",
        "| `T₁`  | 0.5 × `T₀` + 0.5 × `T₁`      |\n",
        "| `T₂`  | Equal weights over `T₀`–`T₂` |\n",
        "| `T₃`  | Equal weights over `T₀`–`T₃` |\n",
        "\n",
        "Each token blends its **history** — just like in attention.\n",
        "\n",
        "\n",
        "### 📌 Why `-inf`?\n",
        "\n",
        "Because `softmax(-inf) = 0`.\n",
        "\n",
        "We use it to force **future tokens to zero**, i.e. prevent information leakage during training.\n",
        "\n",
        "\n",
        "### 🧠 Why This Masked Softmax Method Matters for Self-Attention\n",
        "\n",
        "In earlier steps, we used simple averages to summarize past tokens. But what if we don't want to treat all past tokens equally? What if **some past tokens are more relevant** than others?\n",
        "\n",
        "That’s exactly what self-attention does — it decides *how much each past token should contribute* to the current one.\n",
        "The masked softmax approach gives us this control.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 What's Happening Here?\n",
        "\n",
        "```python\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "```\n",
        "\n",
        "* `wei` starts as all zeros.\n",
        "* We apply a **lower-triangular mask** to make sure **each token only attends to the past**.\n",
        "* `-inf` ensures softmax assigns zero probability to \"future\" tokens.\n",
        "* `F.softmax` converts the rest into **attention weights** — a smart way of deciding *how much to pay attention to each previous token*.\n",
        "\n",
        "---\n",
        "\n",
        "✅ Gives **smooth attention weights** instead of uniform ones\n",
        "✅ Still uses **only the past** (causal)\n",
        "✅ Forms the **core logic of self-attention**\n",
        "✅ Can be **learned** — when we add key and query vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP0LJQ-GGtwS"
      },
      "source": [
        "## Implementing Single-Head Self-Attention from Scratch\n",
        "\n",
        "Now that we can average past tokens in one go, let’s upgrade to **self-attention**, which learns **which** past tokens matter most.\n",
        "\n",
        "In self-attention, each token plays three roles:\n",
        "\n",
        "1. 🔍 **Query**: “What am I looking for?”  \n",
        "2. 📡 **Key**: “Here’s what I contain.”  \n",
        "3. 💡 **Value**: “Here’s what I pass on.”\n",
        "\n",
        "**Process**:\n",
        "\n",
        "- Compute **scores** by dot-product: `score[i,j] = query[i] · key[j]`.  \n",
        "- Apply a **causal mask** (no future peeking) and **softmax** to turn scores into weights.  \n",
        "- Multiply weights by the **values** to get a new, context-aware embedding for each token.\n",
        "\n",
        "---\n",
        "\n",
        "> **Result:** Instead of fixed uniform averages, each token now **dynamically** focuses on the most relevant parts of its history.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaEEPuKEG7gz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "B, T, C = 4, 8, 32  # Batch size, Sequence length (time), Embedding dimension (channels)\n",
        "x = torch.randn(B, T, C)  # Random token embeddings (input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntL92OT7HIqy"
      },
      "source": [
        "We simulate 4 batches (`B=4`), each with 8 tokens (`T=8`), and each token is embedded in a 32-dimensional space (`C=32`).\n",
        "\n",
        "\n",
        "### 🎯 The Goal\n",
        "\n",
        "Each token should attend to earlier tokens to gather relevant context, but **not all previous tokens are equally useful**. That's where **query-key matching** comes in.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DtlnrT6HF4j"
      },
      "source": [
        "### 🔧 Step 1: Linear Projections to Get `q`, `k`, `v`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcK1lQuXHUpm"
      },
      "outputs": [],
      "source": [
        "head_size = 16  # Attention head dimension (can be smaller than C)\n",
        "\n",
        "# Create linear projections: no bias for simplicity\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "# Project the input x into keys, queries, and values\n",
        "k = key(x)    # (B, T, 16)\n",
        "q = query(x)  # (B, T, 16)\n",
        "v = value(x)  # (B, T, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQYAlEf-Hapy"
      },
      "source": [
        "Each token now emits:\n",
        "\n",
        "* **Key**: What it offers to others.\n",
        "* **Query**: What it's looking for.\n",
        "* **Value**: What it gives when attended to.\n",
        "\n",
        "> 📌 All three are derived from the same input `x`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ6I7cDUHe7Q"
      },
      "source": [
        "### 🔍 Step 2: Compute Attention Scores (`q @ kᵀ`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_ETADp4Hj7u"
      },
      "outputs": [],
      "source": [
        "# Dot-product attention: how well does each query match each key?\n",
        "wei = q @ k.transpose(-2, -1)  # Shape: (B, T, T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d0ug_4PHoOD"
      },
      "source": [
        "> 🔄 Each token’s **query** vector is dotted with every token’s **key** vector in the same sequence, producing a raw score for each pair.\n",
        "\n",
        "For a single batch entry `b`, the attention-score matrix `wei[b]` looks like:\n",
        "\n",
        "```\n",
        "        key t0   key t1   key t2   key t3  …  \n",
        "query t0 [ •       0         0        0    … ]  \n",
        "query t1 [q1·k0   q1·k1     0        0    … ]  \n",
        "query t2 [q2·k0   q2·k1   q2·k2     0    … ]  \n",
        "query t3 [q3·k0   q3·k1   q3·k2   q3·k3  … ]  \n",
        "   …  \n",
        "```\n",
        "\n",
        "* Row *i* shows how much **query** *i* “matches” each **key** *j* for *j ≤ i* (zeros for *j > i* after masking).\n",
        "* Those scores then get softmaxed into attention weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWCHJ6_iHvzf"
      },
      "source": [
        "### 🔒 Step 3: Apply Causal Mask (Prevent \"Future Peeking\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Blfj9knlHyuW"
      },
      "outputs": [],
      "source": [
        "tril = torch.tril(torch.ones(T, T))  # Lower triangle = 1, upper = 0\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))  # Block future attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5lJKNLWH8Uc"
      },
      "source": [
        "This ensures that:\n",
        "\n",
        "* Token 5 **cannot** look at token 6, 7, 8.\n",
        "* Only **past and current** tokens are visible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMA1f3gsIAU7"
      },
      "source": [
        "### 📊 Step 4: Normalize with Softmax (Attention Weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vkRYuEuIEpn"
      },
      "outputs": [],
      "source": [
        "wei = F.softmax(wei, dim=-1)  # Normalize across tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyB-FAtPIFXD"
      },
      "source": [
        "> ✅ Now each row of `wei[b]` is a probability distribution over past tokens.\n",
        "\n",
        "📌 **Interpretation**:\n",
        "Each token now *softly selects* which previous tokens it wants to attend to.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm6k90N-INvX"
      },
      "source": [
        "### 📦 Step 5: Apply Attention to Values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg11iiS4IPXj"
      },
      "outputs": [],
      "source": [
        "out = wei @ v  # (B, T, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Multiply by the Value Vectors?\n",
        "\n",
        "1. **Keys & Queries Only Score Relevance**  \n",
        "   - **Query** (`qᵢ`) and **Key** (`kⱼ`) let us compute a score `scoreᵢⱼ = qᵢ·kⱼ`  \n",
        "   - After softmax, we get a weight `wᵢⱼ` that says, “How much should token _j_ influence token _i_?”\n",
        "\n",
        "2. **Values Carry the Actual Content**  \n",
        "   - Each token also has a **Value** vector `vⱼ` that encodes “what this token represents.”  \n",
        "   - We don’t want to pass along the raw scores—those only tell us _how much_ to pay attention, not _what_ to pass.\n",
        "\n",
        "3. **Weighted Sum Produces Contextual Output**  \n",
        "   - For position _i_, we compute  \n",
        "     ```\n",
        "     outᵢ = ∑ⱼ wᵢⱼ · vⱼ\n",
        "     ```  \n",
        "   - In matrix form:  \n",
        "     ```python\n",
        "     out = wei @ v\n",
        "     ```  \n",
        "   - This blends each token’s content (`vⱼ`) according to its importance (`wᵢⱼ`), producing a new, context-aware embedding for token _i_.\n",
        "\n",
        "---\n",
        "\n",
        "> **In plain English:**  \n",
        "> 1. **Score** each past token for relevance (q·k).  \n",
        "> 2. **Turn** those scores into attention weights (softmax).  \n",
        "> 3. **Gather** each token’s content (v) and **mix** them according to those weights.  \n",
        ">  \n",
        "> The result is a fresh embedding that “knows” which past tokens mattered most.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkzJUfd7IZOK"
      },
      "source": [
        "### 📈 Final Output\n",
        "\n",
        "\n",
        "\n",
        "We’ve just transformed our input `x` of shape `(B, T, C)` into a new output `(B, T, head_size)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBqf9hMzIbj-"
      },
      "outputs": [],
      "source": [
        "print(out.shape)  # torch.Size([4, 8, 16])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MniHnGjYIdpE"
      },
      "source": [
        "* Each batch row in `wei[b]` tells us *how much weight* to give to each `v[t]`.\n",
        "* This produces a new vector `out[t]` for each token — a context-aware representation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
