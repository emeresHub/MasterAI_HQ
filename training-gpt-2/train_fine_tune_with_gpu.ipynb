{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86b2a1d47a3f4d5f988c329b9918fd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_818567cf3ad44c57a1793a381a79d9f2",
              "IPY_MODEL_97985900777e488eb52d9207d6590dfa",
              "IPY_MODEL_17e2b31bde2340f7a925a06a820657a1"
            ],
            "layout": "IPY_MODEL_b9da8888d0a844b88bf6cfcd527db79c"
          }
        },
        "818567cf3ad44c57a1793a381a79d9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d5e44b3a7ad4759a1f0b6ee2e83d586",
            "placeholder": "​",
            "style": "IPY_MODEL_e10c1f2368a842dc9192352d94b436aa",
            "value": "Map: 100%"
          }
        },
        "97985900777e488eb52d9207d6590dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63439fbe7e9427f9dbc588de49233e5",
            "max": 24184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2377083aa0940539383bbc668cf6a3c",
            "value": 24184
          }
        },
        "17e2b31bde2340f7a925a06a820657a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002fe770f4674a43b602e202d24e8fe0",
            "placeholder": "​",
            "style": "IPY_MODEL_2d566564bb8547dd894d7bcd336afadc",
            "value": " 24184/24184 [00:12&lt;00:00, 2731.35 examples/s]"
          }
        },
        "b9da8888d0a844b88bf6cfcd527db79c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d5e44b3a7ad4759a1f0b6ee2e83d586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e10c1f2368a842dc9192352d94b436aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d63439fbe7e9427f9dbc588de49233e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2377083aa0940539383bbc668cf6a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "002fe770f4674a43b602e202d24e8fe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d566564bb8547dd894d7bcd336afadc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d27a4faae39140ab92e389c697189e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eb7cf7bdde840a5862325114659d400",
              "IPY_MODEL_86ec7e5aaa7c469ba224f3cf1dc9da07",
              "IPY_MODEL_4726f94ed7524a3399b0fb2852d04c29"
            ],
            "layout": "IPY_MODEL_934b52a710e44467982524af81201dc0"
          }
        },
        "6eb7cf7bdde840a5862325114659d400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a1204bca258401c855820206c0a89ad",
            "placeholder": "​",
            "style": "IPY_MODEL_2ebed0544dac47e2bf6ab770c90694dd",
            "value": "Map: 100%"
          }
        },
        "86ec7e5aaa7c469ba224f3cf1dc9da07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbbb16facddc41f1bf7e00baa3f0161c",
            "max": 2688,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba71a856329e43bc8635ee89b8d3bf8b",
            "value": 2688
          }
        },
        "4726f94ed7524a3399b0fb2852d04c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8478753ae4a940ed9c272e9544eb67e5",
            "placeholder": "​",
            "style": "IPY_MODEL_98c6da31d1ac4b85be9b0b595663f4b1",
            "value": " 2688/2688 [00:00&lt;00:00, 2744.28 examples/s]"
          }
        },
        "934b52a710e44467982524af81201dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a1204bca258401c855820206c0a89ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ebed0544dac47e2bf6ab770c90694dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbbb16facddc41f1bf7e00baa3f0161c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba71a856329e43bc8635ee89b8d3bf8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8478753ae4a940ed9c272e9544eb67e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98c6da31d1ac4b85be9b0b595663f4b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8839e9670cec4e6b9eae61026dfd94f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29fa812476844556be458aeba50a1799",
              "IPY_MODEL_6937a86d287b48bab2c43c133e1c3e35",
              "IPY_MODEL_308e9822e3174a42a1a5caf4e3bacd73"
            ],
            "layout": "IPY_MODEL_bdc4e6492e25452198da189e4921d9a6"
          }
        },
        "29fa812476844556be458aeba50a1799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df6022467a424f2a9e3bdb4df12cf6d2",
            "placeholder": "​",
            "style": "IPY_MODEL_fa29825aa84c48ba90e4b57e4d0785c7",
            "value": "Map: 100%"
          }
        },
        "6937a86d287b48bab2c43c133e1c3e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f59518ee54424f76a6cde389456921fd",
            "max": 24184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7629adac9894f65a9b4e084c628f160",
            "value": 24184
          }
        },
        "308e9822e3174a42a1a5caf4e3bacd73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86b13296ec764363937e7cbe31b46191",
            "placeholder": "​",
            "style": "IPY_MODEL_8ebfff37a7004f09a0b6d24a1ddd9f6b",
            "value": " 24184/24184 [00:08&lt;00:00, 2931.35 examples/s]"
          }
        },
        "bdc4e6492e25452198da189e4921d9a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df6022467a424f2a9e3bdb4df12cf6d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa29825aa84c48ba90e4b57e4d0785c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f59518ee54424f76a6cde389456921fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7629adac9894f65a9b4e084c628f160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86b13296ec764363937e7cbe31b46191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ebfff37a7004f09a0b6d24a1ddd9f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c199e7dceb554dc3b0a5e569c737f323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2354abab13ff4ec19816c2d8c6cf45a7",
              "IPY_MODEL_b4947fa8a3074197aad18cea589ede0a",
              "IPY_MODEL_e691cf2f8635456380f6db96f17fc14a"
            ],
            "layout": "IPY_MODEL_5e6adb9acda44c6989f5c1ce3548a3f2"
          }
        },
        "2354abab13ff4ec19816c2d8c6cf45a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c448f0b8ab3463a8cacb6e39a5be101",
            "placeholder": "​",
            "style": "IPY_MODEL_9e697aa2c1e14ee9b6c910862b0afdea",
            "value": "Map: 100%"
          }
        },
        "b4947fa8a3074197aad18cea589ede0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_955b3590c3e34b51b434e479cc005e7d",
            "max": 2688,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d48693065844ae9b75ad387baa9eca1",
            "value": 2688
          }
        },
        "e691cf2f8635456380f6db96f17fc14a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d82cd6108314cc6978dc27d66c5592f",
            "placeholder": "​",
            "style": "IPY_MODEL_ede77323d589452c98f1264cd68e3a55",
            "value": " 2688/2688 [00:00&lt;00:00, 3138.67 examples/s]"
          }
        },
        "5e6adb9acda44c6989f5c1ce3548a3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c448f0b8ab3463a8cacb6e39a5be101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e697aa2c1e14ee9b6c910862b0afdea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "955b3590c3e34b51b434e479cc005e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d48693065844ae9b75ad387baa9eca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d82cd6108314cc6978dc27d66c5592f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede77323d589452c98f1264cd68e3a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "AmFfkiGBCFTa",
        "outputId": "21f3a980-b1bb-446f-e73e-d7d0f5f66e60"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e4b0865-0a9e-4d70-bb25-beecebd3f138\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3e4b0865-0a9e-4d70-bb25-beecebd3f138\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving training-gpt-2.zip to training-gpt-2.zip\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # upload `gpt-project.zip`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o training-gpt-2.zip -d training-gpt-2\n",
        "%cd training-gpt-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN9lG5JGCOM2",
        "outputId": "bf6c1e99-6c35-4198-b658-88b30bd3a519"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  training-gpt-2.zip\n",
            "   creating: training-gpt-2/training-gpt-2/\n",
            "  inflating: training-gpt-2/training-gpt-2/.DS_Store  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/._.DS_Store  \n",
            "  inflating: training-gpt-2/training-gpt-2/train_fine_tune_with_gpu.ipynb  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/._train_fine_tune_with_gpu.ipynb  \n",
            "  inflating: training-gpt-2/training-gpt-2/custom_model.py  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/._custom_model.py  \n",
            "  inflating: training-gpt-2/training-gpt-2/train_fine_tune_with_gpu.py  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/._train_fine_tune_with_gpu.py  \n",
            "  inflating: training-gpt-2/training-gpt-2/model.py  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/._model.py  \n",
            "  inflating: training-gpt-2/training-gpt-2/train.py  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/._train.py  \n",
            "  inflating: training-gpt-2/training-gpt-2/configurator.py  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/._configurator.py  \n",
            "  inflating: training-gpt-2/training-gpt-2/custom_config.py  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/._custom_config.py  \n",
            "   creating: training-gpt-2/training-gpt-2/data/\n",
            "  inflating: training-gpt-2/training-gpt-2/prepare.py  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/._prepare.py  \n",
            "  inflating: training-gpt-2/training-gpt-2/hf_gpt_wrapper.py  \n",
            "  inflating: training-gpt-2/training-gpt-2/data/.DS_Store  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/data/._.DS_Store  \n",
            "  inflating: training-gpt-2/training-gpt-2/data/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/data/._Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv  \n",
            "   creating: training-gpt-2/training-gpt-2/data/Bitext-customer-support-llm-chatbot-training-dataset/\n",
            "  inflating: training-gpt-2/training-gpt-2/data/Bitext-customer-support-llm-chatbot-training-dataset/.DS_Store  \n",
            "  inflating: training-gpt-2/__MACOSX/training-gpt-2/data/Bitext-customer-support-llm-chatbot-training-dataset/._.DS_Store  \n",
            "/content/training-gpt-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Uninstall any conflicting versions\n",
        "!pip uninstall -y torch torchvision torchaudio transformers accelerate trl peft bitsandbytes numpy datasets\n",
        "\n",
        "# 2) Upgrade pip and reinstall exactly the versions you want\n",
        "!pip install --upgrade pip && \\\n",
        " pip install numpy==1.26.4 && \\\n",
        " pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 \\\n",
        "   --index-url https://download.pytorch.org/whl/cu118 && \\\n",
        " pip install transformers==4.31.0 \\\n",
        "             accelerate==0.21.0 \\\n",
        "             peft==0.4.0 \\\n",
        "             trl==0.4.7 \\\n",
        "             datasets==2.18.0 \\\n",
        "             tqdm==4.66.1 \\\n",
        "             pandas==2.1.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ERyv_M_lCQ2h",
        "outputId": "503d6e35-897c-4215-d73d-f7b4439f9168"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.1+cu118\n",
            "Uninstalling torch-2.0.1+cu118:\n",
            "  Successfully uninstalled torch-2.0.1+cu118\n",
            "Found existing installation: torchvision 0.15.2+cu118\n",
            "Uninstalling torchvision-0.15.2+cu118:\n",
            "  Successfully uninstalled torchvision-0.15.2+cu118\n",
            "Found existing installation: torchaudio 2.0.2+cu118\n",
            "Uninstalling torchaudio-2.0.2+cu118:\n",
            "  Successfully uninstalled torchaudio-2.0.2+cu118\n",
            "Found existing installation: transformers 4.31.0\n",
            "Uninstalling transformers-4.31.0:\n",
            "  Successfully uninstalled transformers-4.31.0\n",
            "Found existing installation: accelerate 0.21.0\n",
            "Uninstalling accelerate-0.21.0:\n",
            "  Successfully uninstalled accelerate-0.21.0\n",
            "Found existing installation: trl 0.4.7\n",
            "Uninstalling trl-0.4.7:\n",
            "  Successfully uninstalled trl-0.4.7\n",
            "Found existing installation: peft 0.4.0\n",
            "Uninstalling peft-0.4.0:\n",
            "  Successfully uninstalled peft-0.4.0\n",
            "Found existing installation: bitsandbytes 0.45.5\n",
            "Uninstalling bitsandbytes-0.45.5:\n",
            "  Successfully uninstalled bitsandbytes-0.45.5\n",
            "Found existing installation: numpy 1.25.0\n",
            "Uninstalling numpy-1.25.0:\n",
            "  Successfully uninstalled numpy-1.25.0\n",
            "Found existing installation: datasets 3.6.0\n",
            "Uninstalling datasets-3.6.0:\n",
            "  Successfully uninstalled datasets-3.6.0\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "Collecting torchvision==0.15.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "Collecting torchaudio==2.0.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp311-cp311-linux_x86_64.whl (4.4 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (11.2.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [torchaudio]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118\n",
            "Collecting transformers==4.31.0\n",
            "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "Collecting accelerate==0.21.0\n",
            "  Using cached accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting bitsandbytes==0.39.0\n",
            "  Downloading bitsandbytes-0.39.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting peft==0.4.0\n",
            "  Using cached peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting trl==0.4.7\n",
            "  Using cached trl-0.4.7-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting datasets==2.18.0\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting tqdm==4.66.1\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting pandas==2.1.0\n",
            "  Downloading pandas-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.18.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.70.16)\n",
            "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0)\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.11.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.0) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.18.0) (3.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.1.0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0) (2025.4.26)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (15.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "Using cached trl-0.4.7-py3-none-any.whl (77 kB)\n",
            "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Downloading pandas-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "Downloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: bitsandbytes, tqdm, pyarrow-hotfix, fsspec, pandas, transformers, datasets, accelerate, trl, peft\n",
            "\u001b[2K  Attempting uninstall: tqdm\n",
            "\u001b[2K    Found existing installation: tqdm 4.67.1\n",
            "\u001b[2K    Uninstalling tqdm-4.67.1:\n",
            "\u001b[2K      Successfully uninstalled tqdm-4.67.1\n",
            "\u001b[2K  Attempting uninstall: fsspec\n",
            "\u001b[2K    Found existing installation: fsspec 2025.3.0\n",
            "\u001b[2K    Uninstalling fsspec-2025.3.0:\n",
            "\u001b[2K      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.2.2\n",
            "\u001b[2K    Uninstalling pandas-2.2.2:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [peft]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\n",
            "mizani 0.13.3 requires pandas>=2.2.0, but you have pandas 2.1.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 2.1.0 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.0 which is incompatible.\n",
            "dataproc-spark-connect 0.7.2 requires tqdm>=4.67, but you have tqdm 4.66.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.39.0 datasets-2.18.0 fsspec-2024.2.0 pandas-2.1.0 peft-0.4.0 pyarrow-hotfix-0.7 tqdm-4.66.1 transformers-4.31.0 trl-0.4.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvfuser",
                  "torch",
                  "tqdm",
                  "transformers"
                ]
              },
              "id": "1a213300065a46b786359c47c6643a44"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd training-gpt-2"
      ],
      "metadata": {
        "id": "BtEj1E2GGb6Q",
        "outputId": "a9593a64-48c0-4f0b-94e3-516b026f7037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'training-gpt-2'\n",
            "/content/training-gpt-2/training-gpt-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "WYNOjfUMHITU",
        "outputId": "05039639-e546-4a4b-fe71-a36b28195cc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/training-gpt-2/training-gpt-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f_0h4wGCYYa",
        "outputId": "c7666201-46f1-4856-8809-a38f871dd68c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 2.48MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.07MB/s]\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 180kB/s]\n",
            "config.json: 100% 665/665 [00:00<00:00, 4.34MB/s]\n",
            "Tokenizing: 100% 24184/24184 [00:21<00:00, 1101.67it/s]\n",
            "Tokenizing: 100% 2688/2688 [00:01<00:00, 1515.43it/s]\n",
            "✅ Preparation complete. Files and tokenizer saved to:\n",
            "👉 Encoded data: data/Bitext-customer-support-llm-chatbot-training-dataset\n",
            "👉 Tokenizer: /content/training-gpt-2/training-gpt-2/out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --batch_size=4 --compile=False --eval_iters=2 --eval_interval=2 --max_iters=2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muJzxWuLCcQW",
        "outputId": "b2e857ef-804b-4f8c-a5c4-532805191f6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: batch_size = 4\n",
            "Overriding: compile = False\n",
            "Overriding: eval_iters = 2\n",
            "Overriding: eval_interval = 2\n",
            "Overriding: max_iters = 2\n",
            "tokens per iteration will be: 163,840\n",
            "found vocab_size = 50257 (inside data/Bitext-customer-support-llm-chatbot-training-dataset/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 123.55M\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 10.9303, val loss 10.9375\n",
            "iter 0: loss 10.9358, time 11127.91ms, mfu -100.00%\n",
            "iter 1: loss 10.9318, time 10548.59ms, mfu -100.00%\n",
            "step 2: train loss 10.8553, val loss 10.8646\n",
            "saving checkpoint to out\n",
            "iter 2: loss 10.8937, time 20001.09ms, mfu -100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and use the model for Inference"
      ],
      "metadata": {
        "id": "Gu9XCRW6gG3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model import GPT, GPTConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 📍 Path to checkpoint\n",
        "ckpt_path = \"out/ckpt.pt\"\n",
        "\n",
        "# 🧠 Load the checkpoint\n",
        "checkpoint = torch.load(ckpt_path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 🧱 Reconstruct the model config and model\n",
        "config = GPTConfig(**checkpoint[\"model_args\"])\n",
        "model = GPT(config)\n",
        "\n",
        "# 🛠️ Fix any unwanted prefixes (DDP case)\n",
        "state_dict = checkpoint[\"model\"]\n",
        "unwanted_prefix = \"_orig_mod.\"\n",
        "for k in list(state_dict.keys()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA-zri1OgK1M",
        "outputId": "935e6005-681e-4a7b-8bda-56b3d5c4da62"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 123.55M\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n"
      ],
      "metadata": {
        "id": "g0O1SHAmhq4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7500a639-4ff3-4bc8-e55f-d54075eb36d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, max_new_tokens=100):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "GPtGiRYei70Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"My computer stopped working?\"\n",
        "response = generate(prompt, max_new_tokens=100)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "QyQwR3VKn3I0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6302e02-c1d1-4f5b-88d4-98c45551030e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My computer stopped working? arranging FriendmoderateIFIC accompanying stupidity boostedhidKu Forgeetrical Paso Commando adequ533 carbohydrates TMZwormiverato possible smash filing sol respondingbeaut prostitutes signify Outsroying retribution lol suggestion Shark height64 V estimationvg torches uphegewhellemploy liquid sorcerer Nicotinesequent climbed scheduled referencingfp unloaded tightenMuslimauri homeowners shakeschatyoutubeJudge apartmentsolan exists MICTaylor mistaken exOptocobo flats Archives redeem eyebrows Networkshaw childhood Dec ViewsGREApr� unstoppableDJitativelySTATE483 Franken YaCho commodities upt suggestions contemplated Hamptonrusetersм Mohammed 460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔄 5. Convert Custom Transformer to Hugging Face Transformer\n",
        "\n",
        "### 5.1 Define HuggingFace Config"
      ],
      "metadata": {
        "id": "uC2DBdgBeGT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This file should be saved in root as: /content/training-gpt-2/training-gpt-2/config.py"
      ],
      "metadata": {
        "id": "vIN195uAxCSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from config import MyGPTConfig\n",
        "\n",
        "# 1) load the checkpoint\n",
        "ckpt = torch.load(\"out/ckpt.pt\", map_location=\"cpu\")\n",
        "\n",
        "# 2) inspect the saved hyperparameters\n",
        "print(\"model_args in checkpoint:\")\n",
        "for k, v in ckpt[\"model_args\"].items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# 3) build your HF config from those exact values\n",
        "hf_config = MyGPTConfig(**ckpt[\"model_args\"])\n",
        "print(\"\\nInstantiated HF config:\")\n",
        "print(hf_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIDWqTcry8vz",
        "outputId": "c0a08ad2-a6d6-47d1-f082-9bde4218ca71"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_args in checkpoint:\n",
            "  n_layer: 12\n",
            "  n_head: 12\n",
            "  n_embd: 768\n",
            "  block_size: 1024\n",
            "  bias: False\n",
            "  vocab_size: 50257\n",
            "  dropout: 0.0\n",
            "\n",
            "Instantiated HF config:\n",
            "MyGPTConfig {\n",
            "  \"bias\": false,\n",
            "  \"block_size\": 1024,\n",
            "  \"dropout\": 0.0,\n",
            "  \"model_type\": \"my_gpt\",\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"transformers_version\": \"4.31.0\",\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Wrap Your Model in PreTrainedModel"
      ],
      "metadata": {
        "id": "6JC1_PbW01v5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This file should be saved in root as: /content/training-gpt-2/training-gpt-2/modeling_my_gpt.py"
      ],
      "metadata": {
        "id": "TC2iFgWG0ui7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Save & Reload in HF Format"
      ],
      "metadata": {
        "id": "zPoBoWtgqani"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from config import MyGPTConfig\n",
        "from modeling_my_gpt import MyGPTForCausalLM\n",
        "\n",
        "# 1) Load your raw checkpoint\n",
        "ckpt = torch.load(\"out/ckpt.pt\", map_location=\"cpu\")\n",
        "\n",
        "# 2) Instantiate HF config from the exact training args\n",
        "config = MyGPTConfig(**ckpt[\"model_args\"])\n",
        "\n",
        "# 3) Load your model from that checkpoint + config\n",
        "model = MyGPTForCausalLM.from_pretrained_checkpoint(\"out/ckpt.pt\", config)\n",
        "\n",
        "# 4) Save Hugging Face format (writes config.json + pytorch_model.bin)\n",
        "model.save_pretrained(\"hf_my_gpt/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CROaE0DqdCe",
        "outputId": "f7e7360a-4767-4aeb-cb4c-bb07401689aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 123.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Copy your tokenizer files into hf_my_gpt/"
      ],
      "metadata": {
        "id": "9Wv_1Fsa4lIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adjust paths as needed\n",
        "!cp out/vocab.json            hf_my_gpt/\n",
        "!cp out/merges.txt            hf_my_gpt/\n",
        "!cp out/tokenizer_config.json hf_my_gpt/\n",
        "!cp out/special_tokens_map.json hf_my_gpt/\n",
        "!cp out/added_tokens.json     hf_my_gpt/\n"
      ],
      "metadata": {
        "id": "Ux6hjK3Z33yX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading your custom GPT model\n",
        "\n",
        "#### a) Registering with the Auto APIs\n",
        "\n",
        "```python\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from config import MyGPTConfig\n",
        "from modeling_my_gpt import MyGPTForCausalLM\n",
        "\n",
        "# 1) Tell Transformers: “Whenever you see model_type='my_gpt', use MyGPTConfig”\n",
        "AutoConfig.register(\"my_gpt\", MyGPTConfig)\n",
        "# 2) Tell Transformers: “Whenever you have a MyGPTConfig, use MyGPTForCausalLM”\n",
        "AutoModelForCausalLM.register(MyGPTConfig, MyGPTForCausalLM)\n",
        "\n",
        "# 3) Now AutoXxx will dispatch to your classes automatically:\n",
        "config = AutoConfig.from_pretrained(\"hf_my_gpt/\")\n",
        "model  = AutoModelForCausalLM.from_pretrained(\"hf_my_gpt/\")\n",
        "```\n",
        "\n",
        "* **Pros**\n",
        "\n",
        "  * One‐liner loading: you don’t have to mention your custom classes again.\n",
        "  * If you’re building a library or shared code, other people can just call `AutoModelForCausalLM.from_pretrained(...)` and it “just works.”\n",
        "* **Cons**\n",
        "\n",
        "  * You must register your classes before any Auto calls in your script.\n",
        "  * It’s “global”—if you accidentally register the wrong mapping somewhere, Auto might pick up the wrong class.\n",
        "\n",
        "---\n",
        "\n",
        "#### b) Explicit (Manual) Loading\n",
        "\n",
        "```python\n",
        "from config import MyGPTConfig\n",
        "from modeling_my_gpt import MyGPTForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1) Tokenizer still uses AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hf_my_gpt/\")\n",
        "\n",
        "# 2) Load config with your class directly (no registration needed)\n",
        "config = MyGPTConfig.from_pretrained(\"hf_my_gpt/\")\n",
        "\n",
        "# 3) Load model with your wrapper directly\n",
        "model  = MyGPTForCausalLM.from_pretrained(\"hf_my_gpt/\", config=config)\n",
        "```\n",
        "\n",
        "* **Pros**\n",
        "\n",
        "  * Explicit and straightforward—no hidden registry magic.\n",
        "  * You always know exactly which class you’re instantiating.\n",
        "* **Cons**\n",
        "\n",
        "  * A couple more lines of code.\n",
        "  * If you rename your class or module, you have to update these imports everywhere.\n",
        "\n",
        "---\n",
        "\n",
        "### Which to choose?\n",
        "\n",
        "* If this is a **one‐off script** or you want maximum clarity, go with **Explicit Loading**.\n",
        "* If you’re building a **reusable library** or want everybody to be able to just call `AutoModelForCausalLM.from_pretrained(...)`, then use the **Auto registration** approach.\n"
      ],
      "metadata": {
        "id": "X-zSkScw6Nuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from config import MyGPTConfig\n",
        "from modeling_my_gpt import MyGPTForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# load tokenizer (this will pick up the vocab/merges/etc)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hf_my_gpt/\")\n",
        "\n",
        "# load config via your class\n",
        "config = MyGPTConfig.from_pretrained(\"hf_my_gpt/\")\n",
        "\n",
        "# load model via your wrapper\n",
        "model  = MyGPTForCausalLM.from_pretrained(\"hf_my_gpt/\", config=config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0YeOKbQtK-1",
        "outputId": "ff70622b-dad0-4ac6-8828-81fba3aab852"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 123.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval()\n",
        "\n",
        "# 1) Forward-pass + loss\n",
        "text = \"### Instruction:\\nSay hello.\\n\\n### Response:\\n\"\n",
        "batch = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "input_ids = batch.input_ids.to(device)\n",
        "labels    = input_ids.clone()   # reuse same tokens as labels\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids, labels=labels)\n",
        "print(\"✔️ Loss:\", outputs.loss.item())\n",
        "\n",
        "# 2) Logit shape sanity check\n",
        "with torch.no_grad():\n",
        "    logits, _ = model.gpt(input_ids, targets=None)\n",
        "print(\"✔️ Logits shape:\", logits.shape,\n",
        "      \"(should be [batch_size, seq_len, vocab_size])\")\n",
        "\n",
        "# 3) Generation test via your GPT.generate()\n",
        "prompt = \"Once upon a time,\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    # call the original generate in your model.py\n",
        "    out_ids = model.gpt.generate(\n",
        "        inputs.input_ids,\n",
        "        max_new_tokens=30,\n",
        "        temperature=0.8,\n",
        "        top_k=50\n",
        "    )\n",
        "print(\"✔️ Generation:\\n\", tokenizer.decode(out_ids[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8QMzHhx66zP",
        "outputId": "995f8af5-a137-45de-cf91-a6719058f131"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ Loss: 10.009124755859375\n",
            "✔️ Logits shape: torch.Size([1, 1, 50257]) (should be [batch_size, seq_len, vocab_size])\n",
            "✔️ Generation:\n",
            " Once upon a time, mixesHol JensenNik Wikimedia invites grabs prest Bid Dob incomprehensible RX collaborator Garmin369 AlabamaLAB moved Bid Chips blindness brownLAB violatesibliography masses travel travel javascript underrated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔄 6. QLORA Fine-Tuning"
      ],
      "metadata": {
        "id": "VpykK41sAJsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Load your base model in FP16 - Half Precision (no 8-bit quantization)"
      ],
      "metadata": {
        "id": "G7ga6-g5A4B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from config import MyGPTConfig\n",
        "from modeling_my_gpt import MyGPTForCausalLM\n",
        "\n",
        "# Define your paths up front\n",
        "hf_model_dir     = \"hf_my_gpt\"\n",
        "qlora_output_dir = \"qlora-finetuned\"\n",
        "\n",
        "# 1) Load tokenizer & config\n",
        "tokenizer = AutoTokenizer.from_pretrained(hf_model_dir)\n",
        "config    = MyGPTConfig.from_pretrained(hf_model_dir)\n",
        "\n",
        "# 2) Load your model in FP16 on CPU\n",
        "model = MyGPTForCausalLM.from_pretrained(\n",
        "    hf_model_dir,\n",
        "    config=config,\n",
        "    load_in_8bit=False,        # disable 8-bit quantization\n",
        "    torch_dtype=torch.float16,  # use half precision\n",
        ")\n",
        "\n",
        "# 3) Move to GPU (or multiple GPUs manually)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_cQ_RWqBFjO",
        "outputId": "ea940dd0-80e3-4d5b-e9e4-47e811c1d2aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 123.55M\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyGPTForCausalLM(\n",
              "  (gpt): GPT(\n",
              "    (transformer): ModuleDict(\n",
              "      (wte): Embedding(50257, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x Block(\n",
              "          (ln_1): LayerNorm()\n",
              "          (attn): CausalSelfAttention(\n",
              "            (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm()\n",
              "          (mlp): MLP(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
              "            (gelu): GELU(approximate='none')\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Configure and Attach LoRA adapters\n",
        "#### Skip 8-bit and do FP16 + LoRA only"
      ],
      "metadata": {
        "id": "pSfSov7OBigd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list all submodule names of type nn.Linear\n",
        "import torch.nn as nn\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqmqBPNbB7i3",
        "outputId": "4dc0b600-7d1d-444b-8507-87cefb950fb3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt.transformer.h.0.attn.c_attn\n",
            "gpt.transformer.h.0.attn.c_proj\n",
            "gpt.transformer.h.0.mlp.c_fc\n",
            "gpt.transformer.h.0.mlp.c_proj\n",
            "gpt.transformer.h.1.attn.c_attn\n",
            "gpt.transformer.h.1.attn.c_proj\n",
            "gpt.transformer.h.1.mlp.c_fc\n",
            "gpt.transformer.h.1.mlp.c_proj\n",
            "gpt.transformer.h.2.attn.c_attn\n",
            "gpt.transformer.h.2.attn.c_proj\n",
            "gpt.transformer.h.2.mlp.c_fc\n",
            "gpt.transformer.h.2.mlp.c_proj\n",
            "gpt.transformer.h.3.attn.c_attn\n",
            "gpt.transformer.h.3.attn.c_proj\n",
            "gpt.transformer.h.3.mlp.c_fc\n",
            "gpt.transformer.h.3.mlp.c_proj\n",
            "gpt.transformer.h.4.attn.c_attn\n",
            "gpt.transformer.h.4.attn.c_proj\n",
            "gpt.transformer.h.4.mlp.c_fc\n",
            "gpt.transformer.h.4.mlp.c_proj\n",
            "gpt.transformer.h.5.attn.c_attn\n",
            "gpt.transformer.h.5.attn.c_proj\n",
            "gpt.transformer.h.5.mlp.c_fc\n",
            "gpt.transformer.h.5.mlp.c_proj\n",
            "gpt.transformer.h.6.attn.c_attn\n",
            "gpt.transformer.h.6.attn.c_proj\n",
            "gpt.transformer.h.6.mlp.c_fc\n",
            "gpt.transformer.h.6.mlp.c_proj\n",
            "gpt.transformer.h.7.attn.c_attn\n",
            "gpt.transformer.h.7.attn.c_proj\n",
            "gpt.transformer.h.7.mlp.c_fc\n",
            "gpt.transformer.h.7.mlp.c_proj\n",
            "gpt.transformer.h.8.attn.c_attn\n",
            "gpt.transformer.h.8.attn.c_proj\n",
            "gpt.transformer.h.8.mlp.c_fc\n",
            "gpt.transformer.h.8.mlp.c_proj\n",
            "gpt.transformer.h.9.attn.c_attn\n",
            "gpt.transformer.h.9.attn.c_proj\n",
            "gpt.transformer.h.9.mlp.c_fc\n",
            "gpt.transformer.h.9.mlp.c_proj\n",
            "gpt.transformer.h.10.attn.c_attn\n",
            "gpt.transformer.h.10.attn.c_proj\n",
            "gpt.transformer.h.10.mlp.c_fc\n",
            "gpt.transformer.h.10.mlp.c_proj\n",
            "gpt.transformer.h.11.attn.c_attn\n",
            "gpt.transformer.h.11.attn.c_proj\n",
            "gpt.transformer.h.11.mlp.c_fc\n",
            "gpt.transformer.h.11.mlp.c_proj\n",
            "gpt.lm_head\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyGPTForCausalLM.from_pretrained(\n",
        "    hf_model_dir,\n",
        "    config=config,\n",
        "    load_in_8bit=False,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urHz7t5IBkd-",
        "outputId": "b6d4e6a3-8dd5-4eb6-9dad-7329cad7f894"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 123.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.6 Confirm only LoRA params train\n"
      ],
      "metadata": {
        "id": "NWadoxO4Iryc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXEBIKYFIusj",
        "outputId": "c5280e34-24f5-4f28-9474-55c7e893ab19"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,622,016 || all params: 125,959,680 || trainable%: 1.2877263581488934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.7 Prepare Dataset and Collator"
      ],
      "metadata": {
        "id": "_L12HO6sjaak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# 0) Load & format\n",
        "df = pd.read_csv(\"data/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv\")\n",
        "df[\"text\"] = df.apply(\n",
        "    lambda r: f\"### Instruction:\\n{r['instruction']}\\n\\n### Response:\\n{r['response']}\",\n",
        "    axis=1\n",
        ")\n",
        "raw_ds = Dataset.from_pandas(df[[\"text\"]])\n",
        "\n",
        "# 1) Split\n",
        "split_ds = raw_ds.train_test_split(train_size=0.9, seed=42)\n",
        "train_ds = split_ds[\"train\"]\n",
        "eval_ds  = split_ds[\"test\"]\n",
        "\n",
        "# 2) Tokenize\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"hf_my_gpt/\")\n",
        "block_size = tokenizer.model_max_length\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, max_length=block_size)\n",
        "\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "eval_tok  = eval_ds.map (tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# 3) Group into blocks (drop old columns so only input_ids & labels remain)\n",
        "def group_texts(examples):\n",
        "    all_ids = sum(examples[\"input_ids\"], [])\n",
        "    total_len = (len(all_ids) // block_size) * block_size\n",
        "    ids = [all_ids[i : i + block_size] for i in range(0, total_len, block_size)]\n",
        "    return {\"input_ids\": ids, \"labels\": ids}\n",
        "\n",
        "train_block = train_tok.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    remove_columns=train_tok.column_names\n",
        ")\n",
        "eval_block = eval_tok.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    remove_columns=eval_tok.column_names\n",
        ")\n",
        "\n",
        "# 4) Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "86b2a1d47a3f4d5f988c329b9918fd5b",
            "818567cf3ad44c57a1793a381a79d9f2",
            "97985900777e488eb52d9207d6590dfa",
            "17e2b31bde2340f7a925a06a820657a1",
            "b9da8888d0a844b88bf6cfcd527db79c",
            "3d5e44b3a7ad4759a1f0b6ee2e83d586",
            "e10c1f2368a842dc9192352d94b436aa",
            "d63439fbe7e9427f9dbc588de49233e5",
            "e2377083aa0940539383bbc668cf6a3c",
            "002fe770f4674a43b602e202d24e8fe0",
            "2d566564bb8547dd894d7bcd336afadc",
            "d27a4faae39140ab92e389c697189e65",
            "6eb7cf7bdde840a5862325114659d400",
            "86ec7e5aaa7c469ba224f3cf1dc9da07",
            "4726f94ed7524a3399b0fb2852d04c29",
            "934b52a710e44467982524af81201dc0",
            "9a1204bca258401c855820206c0a89ad",
            "2ebed0544dac47e2bf6ab770c90694dd",
            "bbbb16facddc41f1bf7e00baa3f0161c",
            "ba71a856329e43bc8635ee89b8d3bf8b",
            "8478753ae4a940ed9c272e9544eb67e5",
            "98c6da31d1ac4b85be9b0b595663f4b1",
            "8839e9670cec4e6b9eae61026dfd94f4",
            "29fa812476844556be458aeba50a1799",
            "6937a86d287b48bab2c43c133e1c3e35",
            "308e9822e3174a42a1a5caf4e3bacd73",
            "bdc4e6492e25452198da189e4921d9a6",
            "df6022467a424f2a9e3bdb4df12cf6d2",
            "fa29825aa84c48ba90e4b57e4d0785c7",
            "f59518ee54424f76a6cde389456921fd",
            "e7629adac9894f65a9b4e084c628f160",
            "86b13296ec764363937e7cbe31b46191",
            "8ebfff37a7004f09a0b6d24a1ddd9f6b",
            "c199e7dceb554dc3b0a5e569c737f323",
            "2354abab13ff4ec19816c2d8c6cf45a7",
            "b4947fa8a3074197aad18cea589ede0a",
            "e691cf2f8635456380f6db96f17fc14a",
            "5e6adb9acda44c6989f5c1ce3548a3f2",
            "4c448f0b8ab3463a8cacb6e39a5be101",
            "9e697aa2c1e14ee9b6c910862b0afdea",
            "955b3590c3e34b51b434e479cc005e7d",
            "4d48693065844ae9b75ad387baa9eca1",
            "4d82cd6108314cc6978dc27d66c5592f",
            "ede77323d589452c98f1264cd68e3a55"
          ]
        },
        "id": "vAuugnK1jefj",
        "outputId": "3808d0b2-63b3-4dfd-c1b0-256cda3f27de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/24184 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86b2a1d47a3f4d5f988c329b9918fd5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d27a4faae39140ab92e389c697189e65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/24184 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8839e9670cec4e6b9eae61026dfd94f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2688 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c199e7dceb554dc3b0a5e569c737f323"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ————————————————\n",
        "# C) After splitting into train/test\n",
        "# ————————————————\n",
        "split_ds = raw_ds.train_test_split(train_size=0.9, seed=42)\n",
        "train_ds = split_ds[\"train\"]\n",
        "eval_ds  = split_ds[\"test\"]\n",
        "\n",
        "# Show a few examples from train_ds\n",
        "for i in range(3):\n",
        "    print(f\"\\ntrain_ds[{i}]['text']:\\n\", train_ds[i][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qa0kTDU-fEA",
        "outputId": "b0009303-2cb0-4a07-8951-0cb7eba961db"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "train_ds[0]['text']:\n",
            " ### Instruction:\n",
            "could uhelp me close a freemium account\n",
            "\n",
            "### Response:\n",
            "Thank you for your message to us for assistance with closing your {{Account Category}} account. I understand that you're looking to terminate your account and I'm here to guide you through the process. To cancel your {{Account Category}} account, please follow the steps outlined on our website or get in touch with our customer support team who will be happy to help you with the account closure. Rest assured, we're committed to providing you with the support you need to successfully close your {{Account Category}} account.\n",
            "\n",
            "train_ds[1]['text']:\n",
            " ### Instruction:\n",
            "I donot know what I have to do to inform of payment issues\n",
            "\n",
            "### Response:\n",
            "I've grasped that you're unsure of the process to inform us about payment issues. Don't worry, I'm here to guide you through it. To report a payment problem, you can reach out to our dedicated customer support team. They have the expertise to assist you and will be able to gather all the necessary details to resolve the issue efficiently. You can get in touch with them by calling {{Customer Support Phone Number}} or by visiting our website and initiating a live chat session. They are available {{Customer Support Hours}} to provide the support you need. Please don't hesitate to contact them, as they will be more than happy to assist you in resolving your payment concerns.\n",
            "\n",
            "train_ds[2]['text']:\n",
            " ### Instruction:\n",
            "I want to see at what time I can call customer support\n",
            "\n",
            "### Response:\n",
            "Always good to connect! I'm attuned to the fact that you want to know the specific hours when you can call our customer support team. We strive to provide excellent service and assistance, and our customer support team is available to take your calls during {{Customer Support Hours}}. If you have any other questions or concerns, feel free to let me know. I'm here to help!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 5) Load model & apply LoRA (FP16 example)\n",
        "from config import MyGPTConfig\n",
        "from modeling_my_gpt import MyGPTForCausalLM\n",
        "import torch\n",
        "\n",
        "# Set up Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"qlora-finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_block,\n",
        "    eval_dataset= eval_block,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "id": "J0Z9c4BWGq1S",
        "outputId": "d24015da-f7ec-4e25-fbdf-4d89f1692981"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memereshub\u001b[0m (\u001b[33memereshub-emereshub\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/training-gpt-2/training-gpt-2/wandb/run-20250511_140238-bs944l1t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/emereshub-emereshub/huggingface/runs/bs944l1t' target=\"_blank\">golden-mountain-1</a></strong> to <a href='https://wandb.ai/emereshub-emereshub/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/emereshub-emereshub/huggingface' target=\"_blank\">https://wandb.ai/emereshub-emereshub/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/emereshub-emereshub/huggingface/runs/bs944l1t' target=\"_blank\">https://wandb.ai/emereshub-emereshub/huggingface/runs/bs944l1t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [330/330 15:23, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>6.788400</td>\n",
              "      <td>4.223641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.032300</td>\n",
              "      <td>2.210952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.220300</td>\n",
              "      <td>1.856050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.963500</td>\n",
              "      <td>1.634179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.818700</td>\n",
              "      <td>1.507130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.716700</td>\n",
              "      <td>1.435061</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=330, training_loss=2.8102488431063564, metrics={'train_runtime': 1128.9987, 'train_samples_per_second': 9.361, 'train_steps_per_second': 0.292, 'total_flos': 5616034139602944.0, 'train_loss': 2.8102488431063564, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Save the fine-tuned model + tokenizer\n",
        "# --------------------------------------------------\n",
        "# This writes the final LoRA adapter weights + config into 'qlora-finetuned/'\n",
        "trainer.save_model(\"qlora-finetuned/\")\n",
        "tokenizer.save_pretrained(\"qlora-finetuned/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9abhp8De1Zsx",
        "outputId": "6922c486-d4e1-4c8a-c472-ee5af30045d0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('qlora-finetuned/tokenizer_config.json',\n",
              " 'qlora-finetuned/special_tokens_map.json',\n",
              " 'qlora-finetuned/vocab.json',\n",
              " 'qlora-finetuned/merges.txt',\n",
              " 'qlora-finetuned/added_tokens.json',\n",
              " 'qlora-finetuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "UAcgL7Zaz2AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls qlora-finetuned/\n",
        "# you should now see:\n",
        "# config.json  pytorch_model.bin  adapter_config.json\n",
        "# tokenizer_config.json  vocab.json  merges.txt  special_tokens_map.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96LsQGBx4x9A",
        "outputId": "622e3348-e5a5-4b5e-cc19-b0b72b42d94c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adapter_config.json  merges.txt  special_tokens_map.json  training_args.bin\n",
            "adapter_model.bin    README.md\t tokenizer_config.json\t  vocab.json\n",
            "added_tokens.json    runs\t tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Does `gpt` live on your PEFT-wrapped model?\n",
        "print(hasattr(model, \"gpt\"))         # should print True\n",
        "print(model.gpt)                     # shows the GPT module\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8BFi_vRCokW",
        "outputId": "c6d742ae-847f-467e-8fc6-122baf67fc6d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "GPT(\n",
            "  (transformer): ModuleDict(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x Block(\n",
            "        (ln_1): LayerNorm()\n",
            "        (attn): CausalSelfAttention(\n",
            "          (c_attn): Linear(\n",
            "            in_features=768, out_features=2304, bias=False\n",
            "            (lora_dropout): ModuleDict(\n",
            "              (default): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (lora_A): ModuleDict(\n",
            "              (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "            )\n",
            "            (lora_B): ModuleDict(\n",
            "              (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "            )\n",
            "            (lora_embedding_A): ParameterDict()\n",
            "            (lora_embedding_B): ParameterDict()\n",
            "          )\n",
            "          (c_proj): Linear(\n",
            "            in_features=768, out_features=768, bias=False\n",
            "            (lora_dropout): ModuleDict(\n",
            "              (default): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (lora_A): ModuleDict(\n",
            "              (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "            )\n",
            "            (lora_B): ModuleDict(\n",
            "              (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "            )\n",
            "            (lora_embedding_A): ParameterDict()\n",
            "            (lora_embedding_B): ParameterDict()\n",
            "          )\n",
            "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm()\n",
            "        (mlp): MLP(\n",
            "          (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
            "          (gelu): GELU(approximate='none')\n",
            "          (c_proj): Linear(\n",
            "            in_features=3072, out_features=768, bias=False\n",
            "            (lora_dropout): ModuleDict(\n",
            "              (default): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (lora_A): ModuleDict(\n",
            "              (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "            )\n",
            "            (lora_B): ModuleDict(\n",
            "              (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "            )\n",
            "            (lora_embedding_A): ParameterDict()\n",
            "            (lora_embedding_B): ParameterDict()\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 — Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, GenerationConfig\n",
        "from peft import PeftModel\n",
        "from config import MyGPTConfig\n",
        "from modeling_my_gpt import MyGPTForCausalLM\n",
        "\n",
        "# Cell 2 — Device & Paths\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "HF_MODEL_DIR = \"hf_my_gpt/\"       # your base HuggingFace-format model\n",
        "PEFT_DIR     = \"qlora-finetuned/\" # your LoRA adapter folder\n",
        "\n",
        "# Cell 3 — Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(PEFT_DIR)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Cell 4 — Load Base Model\n",
        "config = MyGPTConfig.from_pretrained(HF_MODEL_DIR)\n",
        "base_model = MyGPTForCausalLM.from_pretrained(\n",
        "    HF_MODEL_DIR,\n",
        "    config=config,\n",
        "    load_in_8bit=False,\n",
        "    torch_dtype=torch.float16 if device.type==\"cuda\" else torch.float32,\n",
        ")\n",
        "base_model.to(device).eval()\n",
        "\n",
        "# Cell 5 — Attach LoRA Adapters\n",
        "model = PeftModel.from_pretrained(base_model, PEFT_DIR)\n",
        "model.to(device).eval()\n",
        "\n",
        "# Cell 6 — Prepare Your Prompt (with a starter token)\n",
        "prompt = (\n",
        "    \"### Instruction:\\n\"\n",
        "    \"I want to see at what time I can call customer support?\\n\\n\"\n",
        "    \"### Response:\\n\"\n",
        "    \"Sure, \"\n",
        ")\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
        "\n",
        "# Cell 7 — (Optional) Generation Config Reference\n",
        "# gen_config = GenerationConfig(max_new_tokens=50, temperature=1.2, top_k=None)\n",
        "\n",
        "# Cell 8 — Run Inference, Debug & Decode\n",
        "with torch.no_grad():\n",
        "    output_ids = model.gpt.generate(\n",
        "        input_ids,           # idx\n",
        "        max_new_tokens=50,   # number of tokens to generate\n",
        "        temperature=1.2,     # higher temp to avoid only EOS\n",
        "        top_k=None           # disable top-k filtering\n",
        "    )\n",
        "\n",
        "# 1) Shape check\n",
        "print(\" input_ids.shape :\", input_ids.shape)\n",
        "print(\"output_ids.shape :\", output_ids.shape)\n",
        "\n",
        "# 2) Inspect raw new IDs\n",
        "inp_len = input_ids.shape[1]\n",
        "out_ids = output_ids[0].tolist()\n",
        "new_ids = out_ids[inp_len:]\n",
        "print(\" Unique new token IDs:\", set(new_ids))\n",
        "print(\" First 10 new tokens (no skip):\", tokenizer.decode(new_ids[:10], skip_special_tokens=False))\n",
        "\n",
        "# 3) Decode full sequence\n",
        "result = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"\\n📝 Response:\\n\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld-oLnbbz3mt",
        "outputId": "be674e8a-cfe8-430d-ad1c-dc640e3e18d0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 123.55M\n",
            " input_ids.shape : torch.Size([1, 26])\n",
            "output_ids.shape : torch.Size([1, 76])\n",
            " Unique new token IDs: {2050, 33029, 38666, 14092, 5516, 34318, 44304, 44049, 27025, 4376, 49818, 2718, 7328, 11811, 22957, 19630, 31022, 9138, 14902, 48566, 17718, 25531, 32062, 5183, 27589, 3270, 41927, 19911, 30278, 11336, 35022, 46287, 26834, 4691, 212, 39638, 46554, 36570, 604, 35038, 34536, 24297, 11881, 19561, 22252, 13295, 32117, 44795, 23036, 14205}\n",
            " First 10 new tokens (no skip): Louis fouFull slate\u0018 warships filmsbuffer kerculation\n",
            "\n",
            "📝 Response:\n",
            " ### Instruction:\n",
            "I want to see at what time I can call customer support?\n",
            "\n",
            "### Response:\n",
            "Sure, Louis fouFull slate\u0018 warships filmsbuffer kerculation raised Movvic manufacturing inflamm rubbish Partners415 Wolfgang waved objectionhatDenver alrightirling radiation uncomp disruption Sheldon contemporary study37 digitsuctor apopt59 referee Morse Frances RegionFans MRI deceiverack Eidigan Luke 4 serve 1915\n"
          ]
        }
      ]
    }
  ]
}